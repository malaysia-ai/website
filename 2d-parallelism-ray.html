<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- HTML Meta Tags -->
  <title>Malaysia-AI blog 2D Parallelism using Ray PyTorch</title>
  <meta name="description" content="Malaysia-AI blog 2D Parallelism using Ray PyTorch">

  <!-- Facebook Meta Tags -->
  <meta property="og:url" content="https://malaysia-ai.org/2d-parallelism-ray">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Malaysia-AI blog 2D Parallelism using Ray PyTorch">
  <meta property="og:description" content="Malaysia-AI blog 2D Parallelism using Ray PyTorch">
  <meta property="og:image" content="https://malaysia-ai.org/2d-parallelism-ray.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta property="twitter:domain" content="malaysia-ai.org">
  <meta property="twitter:url" content="https://malaysia-ai.org/2d-parallelism-ray">
  <meta name="twitter:title" content="Malaysia-AI blog 2D Parallelism using Ray PyTorch">
  <meta name="twitter:description" content="Malaysia-AI blog 2D Parallelism using Ray PyTorch">
  <meta name="twitter:image" content="https://malaysia-ai.org/2d-parallelism-ray.png">

  <!-- Meta Tags Generated via https://www.opengraph.xyz -->

  <style>
    body {
      line-height: 1.4;
      font-size: 16px;
      padding: 0 10px;
      margin: 50px auto;
      max-width: 1000px;
    }

    #maincontent {
      max-width: 62em;
      margin: 15 auto;
    }

    pre {
      margin-top: 0px;
      white-space: break-spaces;
    }
  </style>
</head>

<body>

  <div id="maincontent" style="margin-top: 70px">
    <h2>2D Parallelism using PyTorch in Ray</h2>
    <p>Last time we already covered <a href="/tp-pytorch">Tensor Parallelism</a>
      using PyTorch Distributed Elastic and little bit of Pipeline Parallelism, but did you know that you can combine
      Tensor Parallelism and Pipeline
      Parallelism in the same parallelism? Actually we got up to 4D! Tensor Parallelism + Pipeline Parallelism + Data
      Parallelism + Context Parallelism, TP + PP + DP + CP! But this blog we will only cover TP and PP.</p>
    <p>As we know, Tensor Parallelism split the weights either Row-Wise or Column-Wise to N GPUs and Pipeline
      Parallelism split hidden layers to N GPUs,</p>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 300">
      <!-- Tensor Parallelism -->
      <g transform="translate(0, 0)">
        <text x="200" y="30" font-size="16" font-weight="bold">Tensor Parallelism (2 GPUs)</text>

        <!-- Input -->
        <rect x="50" y="50" width="60" height="40" fill="#ADD8E6" stroke="black" />
        <text x="80" y="75" font-size="12" text-anchor="middle">Input</text>
        <text x="80" y="85" font-size="10" text-anchor="middle">1x4</text>

        <!-- First Linear Layer -->
        <rect x="150" y="50" width="120" height="40" fill="#90EE90" stroke="black" />
        <text x="210" y="75" font-size="12" text-anchor="middle">Linear 4x4</text>
        <line x1="210" y1="50" x2="210" y2="90" stroke="black" stroke-dasharray="5,5" />
        <text x="175" y="105" font-size="10" text-anchor="end">GPU 0</text>
        <text x="245" y="105" font-size="10" text-anchor="start">GPU 1</text>

        <!-- Second Linear Layer -->
        <rect x="310" y="50" width="120" height="40" fill="#FFA07A" stroke="black" />
        <text x="370" y="75" font-size="12" text-anchor="middle">Linear 4x2</text>
        <line x1="370" y1="50" x2="370" y2="90" stroke="black" stroke-dasharray="5,5" />
        <text x="335" y="105" font-size="10" text-anchor="end">GPU 0</text>
        <text x="405" y="105" font-size="10" text-anchor="start">GPU 1</text>

        <!-- Output -->
        <rect x="470" y="50" width="60" height="40" fill="#FFD700" stroke="black" />
        <text x="500" y="75" font-size="12" text-anchor="middle">Output</text>
        <text x="500" y="85" font-size="10" text-anchor="middle">1x2</text>

        <!-- Connections -->
        <line x1="110" y1="70" x2="150" y2="70" stroke="black" marker-end="url(#arrowhead)" />
        <line x1="270" y1="70" x2="310" y2="70" stroke="black" marker-end="url(#arrowhead)" />
        <line x1="430" y1="70" x2="470" y2="70" stroke="black" marker-end="url(#arrowhead)" />
      </g>

      <!-- Pipeline Parallelism -->
      <g transform="translate(0, 150)">
        <text x="200" y="30" font-size="16" font-weight="bold">Pipeline Parallelism (2 GPUs)</text>

        <!-- GPU 0 -->
        <rect x="50" y="50" width="240" height="80" fill="#E6E6FA" stroke="black" />
        <text x="170" y="70" font-size="14" text-anchor="middle">GPU 0</text>

        <!-- Input -->
        <rect x="70" y="80" width="60" height="40" fill="#ADD8E6" stroke="black" />
        <text x="100" y="105" font-size="12" text-anchor="middle">Input</text>
        <text x="100" y="115" font-size="10" text-anchor="middle">1x4</text>

        <!-- First Linear Layer -->
        <rect x="160" y="80" width="110" height="40" fill="#90EE90" stroke="black" />
        <text x="215" y="105" font-size="12" text-anchor="middle">Linear 4x4</text>

        <!-- GPU 1 -->
        <rect x="330" y="50" width="240" height="80" fill="#FFE4E1" stroke="black" />
        <text x="450" y="70" font-size="14" text-anchor="middle">GPU 1</text>

        <!-- Second Linear Layer -->
        <rect x="350" y="80" width="110" height="40" fill="#FFA07A" stroke="black" />
        <text x="405" y="105" font-size="12" text-anchor="middle">Linear 4x2</text>

        <!-- Output -->
        <rect x="490" y="80" width="60" height="40" fill="#FFD700" stroke="black" />
        <text x="520" y="105" font-size="12" text-anchor="middle">Output</text>
        <text x="520" y="115" font-size="10" text-anchor="middle">1x2</text>

        <!-- Connections -->
        <line x1="130" y1="100" x2="160" y2="100" stroke="black" marker-end="url(#arrowhead)" />
        <line x1="270" y1="100" x2="350" y2="100" stroke="black" marker-end="url(#arrowhead)" />
        <line x1="460" y1="100" x2="490" y2="100" stroke="black" marker-end="url(#arrowhead)" />
      </g>

      <!-- Arrowhead definition -->
      <defs>
        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" />
        </marker>
      </defs>
    </svg>
    <p>We can combine TP and PP to become a single parallelism, called 2D Parallelism. Assumed I have a deep learning
      model
      with 4 hidden layers, and each hidden layer got a linear layer, to make the model fit into 2D Parallelism,</p>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 450">
      <defs>
        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="#000" />
        </marker>
      </defs>

      <!-- GPU Rectangles -->
      <rect x="50" y="50" width="700" height="80" fill="#e6f3ff" stroke="#000" stroke-width="2" />
      <rect x="50" y="150" width="700" height="80" fill="#e6f3ff" stroke="#000" stroke-width="2" />
      <rect x="50" y="250" width="700" height="80" fill="#e6f3ff" stroke="#000" stroke-width="2" />
      <rect x="50" y="350" width="700" height="80" fill="#e6f3ff" stroke="#000" stroke-width="2" />

      <!-- GPU Labels -->
      <text x="60" y="90" font-family="Arial" font-size="20">GPU 0</text>
      <text x="60" y="190" font-family="Arial" font-size="20">GPU 1</text>
      <text x="60" y="290" font-family="Arial" font-size="20">GPU 2</text>
      <text x="60" y="390" font-family="Arial" font-size="20">GPU 3</text>

      <!-- Hidden Layers -->
      <rect x="150" y="60" width="80" height="60" fill="#ffcccc" stroke="#000" />
      <rect x="250" y="60" width="80" height="60" fill="#ffddcc" stroke="#000" />
      <rect x="150" y="160" width="80" height="60" fill="#ffcccc" stroke="#000" />
      <rect x="250" y="160" width="80" height="60" fill="#ffddcc" stroke="#000" />

      <rect x="350" y="260" width="80" height="60" fill="#ffeedd" stroke="#000" />
      <rect x="450" y="260" width="80" height="60" fill="#ffeecc" stroke="#000" />
      <rect x="350" y="360" width="80" height="60" fill="#ffeedd" stroke="#000" />
      <rect x="450" y="360" width="80" height="60" fill="#ffeecc" stroke="#000" />



      <!-- Layer Labels -->
      <text x="170" y="95" font-family="Arial" font-size="12">HL 0</text>
      <text x="270" y="95" font-family="Arial" font-size="12">HL 1</text>
      <text x="170" y="195" font-family="Arial" font-size="12">HL 0</text>
      <text x="270" y="195" font-family="Arial" font-size="12">HL 1</text>

      <text x="370" y="295" font-family="Arial" font-size="12">HL 2</text>
      <text x="470" y="295" font-family="Arial" font-size="12">HL 3</text>
      <text x="370" y="395" font-family="Arial" font-size="12">HL 2</text>
      <text x="470" y="395" font-family="Arial" font-size="12">HL 3</text>

      <!-- Tensor Parallelism Arrows -->
      <line x1="190" y1="120" x2="190" y2="160" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="290" y1="120" x2="290" y2="160" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="390" y1="320" x2="390" y2="360" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="490" y1="320" x2="490" y2="360" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />

      <!-- Pipeline Parallelism Arrows -->
      <line x1="330" y1="120" x2="350" y2="260" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />

      <!-- Legend -->
      <rect x="50" y="10" width="20" height="20" fill="#ffcccc" stroke="#000" />
      <text x="75" y="25" font-family="Arial" font-size="12">HL 0</text>
      <rect x="150" y="10" width="20" height="20" fill="#ffddcc" stroke="#000" />
      <text x="175" y="25" font-family="Arial" font-size="12">HL 1</text>
      <rect x="250" y="10" width="20" height="20" fill="#ffeedd" stroke="#000" />
      <text x="275" y="25" font-family="Arial" font-size="12">HL 2</text>
      <rect x="350" y="10" width="20" height="20" fill="#ffeecc" stroke="#000" />
      <text x="375" y="25" font-family="Arial" font-size="12">HL 3</text>
    </svg>
    <p>- GPU 0 take hidden layers 0-1, this is a PP for hidden layers 0-1, and GPU 0 TP with GPU 1 to shard the weights,
      this can be done using `torch.distributed.new_group`. This required 2 GPUs.</p>
    <p>- Output from hidden layers 0-1 in GPU 0 and will pass to GPU 2, and GPU 2 PP hidden layers 2-3. GPU 2 TP with
      GPU 3 to shard the weights. Also required to create new group using `torch.distributed.new_group`. This required 2
      GPUs.</p>
    <p>- The number of GPUs required is, M PP x N TP, if M = 2 and N = 2, we need 4 GPUs. 1 PP 2 TP means, all
      hidden
      layers inside the same GPU 0, but the weights sharded with GPU 1, so it required 2 GPUs.</p>
    <p>- Because the hidden layers are split across M devices, and each weight is sharded by N, GPU memory is saved by a
      factor of M x N!</p>
    <p>- This 2D Parallelism communication groups are like below,</p>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 450">
      <defs>
        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="#000" />
        </marker>
      </defs>

      <!-- GPU Rectangles -->
      <rect x="50" y="50" width="700" height="80" fill="#e6f3ff" stroke="#000" stroke-width="2" />
      <rect x="50" y="150" width="700" height="80" fill="#e6f3ff" stroke="#000" stroke-width="2" />
      <rect x="50" y="250" width="700" height="80" fill="#e6f3ff" stroke="#000" stroke-width="2" />
      <rect x="50" y="350" width="700" height="80" fill="#e6f3ff" stroke="#000" stroke-width="2" />

      <!-- GPU Labels -->
      <text x="60" y="90" font-family="Arial" font-size="20">GPU 0</text>
      <text x="60" y="190" font-family="Arial" font-size="20">GPU 1</text>
      <text x="60" y="290" font-family="Arial" font-size="20">GPU 2</text>
      <text x="60" y="390" font-family="Arial" font-size="20">GPU 3</text>

      <!-- Hidden Layers -->
      <rect x="150" y="60" width="80" height="60" fill="#ffcccc" stroke="#000" />
      <rect x="250" y="60" width="80" height="60" fill="#ffddcc" stroke="#000" />
      <rect x="150" y="160" width="80" height="60" fill="#ffcccc" stroke="#000" />
      <rect x="250" y="160" width="80" height="60" fill="#ffddcc" stroke="#000" />

      <rect x="350" y="260" width="80" height="60" fill="#ffeedd" stroke="#000" />
      <rect x="450" y="260" width="80" height="60" fill="#ffeecc" stroke="#000" />
      <rect x="350" y="360" width="80" height="60" fill="#ffeedd" stroke="#000" />
      <rect x="450" y="360" width="80" height="60" fill="#ffeecc" stroke="#000" />

      <!-- Layer Labels -->
      <text x="170" y="95" font-family="Arial" font-size="12">HL 0</text>
      <text x="270" y="95" font-family="Arial" font-size="12">HL 1</text>
      <text x="170" y="195" font-family="Arial" font-size="12">HL 0</text>
      <text x="270" y="195" font-family="Arial" font-size="12">HL 1</text>

      <text x="370" y="295" font-family="Arial" font-size="12">HL 2</text>
      <text x="470" y="295" font-family="Arial" font-size="12">HL 3</text>
      <text x="370" y="395" font-family="Arial" font-size="12">HL 2</text>
      <text x="470" y="395" font-family="Arial" font-size="12">HL 3</text>

      <!-- Tensor Parallelism Arrows -->
      <line x1="190" y1="120" x2="190" y2="160" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="290" y1="120" x2="290" y2="160" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="390" y1="320" x2="390" y2="360" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="490" y1="320" x2="490" y2="360" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />

      <!-- Pipeline Parallelism Arrows -->
      <line x1="330" y1="120" x2="350" y2="260" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />

      <!-- New Group Visualizations (Squares) -->
      <!-- TP Group for GPUs 0 and 1 -->
      <rect x="40" y="40" width="320" height="200" fill="none" stroke="#ff0000" stroke-width="2"
        stroke-dasharray="5,5" />
      <text x="70" y="65" font-family="Arial" font-size="14" fill="#ff0000">TP Group: [0, 1]</text>

      <!-- TP Group for GPUs 2 and 3 -->
      <rect x="40" y="240" width="520" height="200" fill="none" stroke="#00ff00" stroke-width="2"
        stroke-dasharray="5,5" />
      <text x="70" y="265" font-family="Arial" font-size="14" fill="#00ff00">TP Group: [2, 3]</text>

      <!-- PP Group for GPUs 0 and 2 -->
      <rect x="30" y="40" width="540" height="400" fill="none" stroke="#0000ff" stroke-width="2"
        stroke-dasharray="5,5" />
      <text x="580" y="65" font-family="Arial" font-size="14" fill="#0000ff">PP Group: [0, 2]</text>

      <!-- Legend -->
      <rect x="50" y="10" width="20" height="20" fill="#ffcccc" stroke="#000" />
      <text x="75" y="25" font-family="Arial" font-size="12">HL 0</text>
      <rect x="150" y="10" width="20" height="20" fill="#ffddcc" stroke="#000" />
      <text x="175" y="25" font-family="Arial" font-size="12">HL 1</text>
      <rect x="250" y="10" width="20" height="20" fill="#ffeedd" stroke="#000" />
      <text x="275" y="25" font-family="Arial" font-size="12">HL 2</text>
      <rect x="350" y="10" width="20" height="20" fill="#ffeecc" stroke="#000" />
      <text x="375" y="25" font-family="Arial" font-size="12">HL 3</text>
    </svg>
    <p>- TP Group: [0, 1] is the TP communication group for GPU 0 and GPU 1, PP Group: [0, 2] is the PP communication
      group for GPU 0 and GPU 2, and TP Group: [2, 3] is the TP communication group for GPU 2 and GPU 3.</p>
    <h3>PyTorch in Ray</h3>
    <p>For distributed framework we decided to use <a href="https://www.ray.io/">Ray</a> <b>(Please sponsor us
        something)</b> because we do
      not have a node with 4 GPUs, but we got 2 nodes with each 2 GPUs, so we connect those nodes using Ray inside
      Tailscale VPN.</p>
    <p>Why Ray? Ray is cool, nice UI, and the important parts are, <b>node auto discovery and automatic distributed
        execution</b>.
    </p>
    <p>What does means by node auto discovery and automatic distributed execution? actually Torch Elastic Distributed
      support multi-nodes natively, you must
      set rendezvous backend, <a
        href="https://pytorch.org/docs/stable/elastic/run.html#note-on-rendezvous-backend">https://pytorch.org/docs/stable/elastic/run.html#note-on-rendezvous-backend</a>
    </p>
    <pre>
```bash
torchrun
--nnodes=$NUM_NODES
--nproc-per-node=$NUM_TRAINERS
--rdzv-id=$JOB_ID
--rdzv-backend=c10d
--rdzv-endpoint=$HOST_NODE_ADDR
YOUR_TRAINING_SCRIPT.py
```</pre>
    <p>- `$NUM_NODES` must set equal to the size of nodes.</p>
    <p>- `$NUM_TRAINERS` must set equal to the size of GPUs.</p>
    <p>- `$JOB_ID` can set any ID, if you have multiple jobs, you must set different ID.</p>
    <p>- `$HOST_NODE_ADDR` is the first node or the fastest node you have, and it will elect as host.</p>
    <p>Now we have 2 nodes and each node got 2 GPUs, with IPs `100.93.25.29` and `100.92.17.27`, so to run using
      torchrun,</p>
    <p>In `100.93.25.29`,</p>
    <pre>
```bash
torchrun \
--nnodes=2 --nproc_per_node=2 \
--rdzv_id=1234 --rdzv_backend=c10d --rdzv_endpoint=100.93.25.29:29500 train.py
```</pre>
    <p>And in `100.92.17.27`, you have to run the same thing,</p>
    <pre>
```bash
torchrun \
--nnodes=2 --nproc_per_node=2 \
--rdzv_id=1234 --rdzv_backend=c10d --rdzv_endpoint=100.93.25.29:29500 train.py
```</pre>
    <p>Which is tedious, and each nodes must have the same script plus you must know the head of IP address! Or maybe
      you saw someone run using Slurm before,
    <pre>
```bash
nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
nodes_array=($nodes)
head_node=${nodes_array[0]}
head_node_ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)

srun torchrun \
--nnodes 2 \
--nproc_per_node 2 \
--rdzv_id 1234 \
--rdzv_backend c10d \
--rdzv_endpoint $head_node_ip:29500 \
train.py
```</pre>
    <p>Slurm also run the script for the entire nodes register in Slurm, but in other to build a Slurm cluster,</p>
    <pre>
```bash
# /etc/slurm-llnl/slurm.conf
ClusterName=my_cluster
ControlMachine=100.93.25.29
# extra configs

NodeName=node1 NodeAddr=100.93.25.29 RealMemory=32000 Sockets=1 CoresPerSocket=4 ThreadsPerCore=2 Gres=gpu:2
NodeName=node2 NodeAddr=100.92.17.27 RealMemory=32000 Sockets=1 CoresPerSocket=4 ThreadsPerCore=2 Gres=gpu:2
PartitionName=debug Nodes=node1,node2 Default=YES MaxTime=INFINITE State=UP
```</pre>
    <p>You need to put the config for the all nodes available, and as you can see, you have to mention all the IP nodes!
    </p>
    <p>But in Ray,
      you do not have to do all of that, you just run the script anywhere as long the script connected to the Ray head
      and Ray
      will automatically distribute the script to another nodes.</p>
    <p>In head node 100.93.25.29, you have to run the Ray head mode,</p>
    <pre>
```bash
ray start --head --node-ip-address=100.93.25.29 --port=6379 --dashboard-host=0.0.0.0
```</pre>
    <p>After that other nodes just connect using,</p>
    <pre>
```bash
ray start --address=100.93.25.29:6379
```</pre>
    <p>Done! The cluster looks like below,</p>
    <div style="width: 50%">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 600 400">
        <!-- Background -->
        <rect x="0" y="0" width="600" height="400" fill="#f0f0f0" />

        <!-- Tailscale VPN -->
        <ellipse cx="300" cy="200" rx="280" ry="180" fill="#e6f3ff" stroke="#2980b9" stroke-width="2" />
        <text x="300" y="50" font-family="Arial, sans-serif" font-size="18" text-anchor="middle"
          fill="#2980b9">Tailscale
          VPN</text>

        <!-- Node 1 -->
        <rect x="100" y="100" width="180" height="200" rx="10" ry="10" fill="#ffffff" stroke="#3498db"
          stroke-width="2" />
        <text x="190" y="130" font-family="Arial, sans-serif" font-size="16" text-anchor="middle">Node 1</text>

        <!-- GPUs in Node 1 -->
        <rect x="120" y="160" width="60" height="40" rx="5" ry="5" fill="#f39c12" />
        <rect x="200" y="160" width="60" height="40" rx="5" ry="5" fill="#f39c12" />
        <text x="150" y="185" font-family="Arial, sans-serif" font-size="14" text-anchor="middle"
          fill="#ffffff">GPU</text>
        <text x="230" y="185" font-family="Arial, sans-serif" font-size="14" text-anchor="middle"
          fill="#ffffff">GPU</text>

        <!-- Node 2 -->
        <rect x="320" y="100" width="180" height="200" rx="10" ry="10" fill="#ffffff" stroke="#3498db"
          stroke-width="2" />
        <text x="410" y="130" font-family="Arial, sans-serif" font-size="16" text-anchor="middle">Node 2</text>

        <!-- GPUs in Node 2 -->
        <rect x="340" y="160" width="60" height="40" rx="5" ry="5" fill="#f39c12" />
        <rect x="420" y="160" width="60" height="40" rx="5" ry="5" fill="#f39c12" />
        <text x="370" y="185" font-family="Arial, sans-serif" font-size="14" text-anchor="middle"
          fill="#ffffff">GPU</text>
        <text x="450" y="185" font-family="Arial, sans-serif" font-size="14" text-anchor="middle"
          fill="#ffffff">GPU</text>

        <!-- Ray connection -->
        <line x1="280" y1="200" x2="320" y2="200" stroke="#27ae60" stroke-width="3" />
        <text x="300" y="230" font-family="Arial, sans-serif" font-size="16" text-anchor="middle"
          fill="#27ae60">Ray</text>

        <!-- Ray logo (simplified) -->
        <circle cx="300" cy="320" r="30" fill="#27ae60" />
        <text x="300" y="328" font-family="Arial, sans-serif" font-size="24" text-anchor="middle" fill="#ffffff"
          font-weight="bold">Ray</text>
      </svg>
    </div>
    <p>Even though to connect to the Ray must use the head node, but all the nodes in the Ray cluster able to
      peer-to-peer communication without need to go the head node. And Ray comes with a nice dashboard!</p>
    <img src="/ray-dashboard.png" width="50%">
    <p>Also natively with Prometheus metrics (but we are not deployed it, too lazy), you can read more at <a
        href="https://www.anyscale.com/blog/monitoring-and-debugging-ray-workloads-ray-metrics">https://www.anyscale.com/blog/monitoring-and-debugging-ray-workloads-ray-metrics</a>,
      so when talk about Prometheus, you can setup real-time alerts to any channels that you want, for an example, GPU
      temp reached >80c so you can send alert to Slack.
    </p>
    <img
      src="https://images.ctfassets.net/xjan103pcp94/1rI8J9Bs7wEOLOCDGdYhmN/c19f8277a8918d6a2c6fa5248906175a/image9.png"
      width="50%">
    <p>Let us look into Ray,</p>
    <pre>
```python
import torch
import torch.nn as nn
import torch.distributed as dist
import os
import ray
from ray import train
from ray.train import ScalingConfig
from ray.train.torch import TorchTrainer

def func():
    print(os.environ['LOCAL_RANK'], os.environ['RANK'], os.environ["WORLD_SIZE"], os.environ["NODE_RANK"])

def main():
    ray.init(address="ray://localhost:10001")
    scaling_config = ScalingConfig(
        num_workers=4,
        use_gpu=True,
    )
    ray_trainer = TorchTrainer(
        func,
        scaling_config=scaling_config,
    )
    ray_trainer.fit()

if __name__ == "__main__":
    main()
```</pre>
    <p>And save it as `test-ray.py`. If you have 4 GPUs, set `num_workers=4`, one worker equal to one GPU if
      `use_gpu=True`. In order to use PyTorch Distributed in Ray, you must use `TorchTrainer`. If you
      look at the source code of
      `TorchTrainer`, <a
        href="https://github.com/ray-project/ray/blob/master/python/ray/train/torch/config.py#L153">https://github.com/ray-project/ray/blob/master/python/ray/train/torch/config.py#L153</a>,
      behind the scene it still use native `torch.distributed.run` and properly setup the `MASTER_ADDR`, <a
        href="https://github.com/ray-project/ray/blob/master/python/ray/train/torch/config.py#L169">https://github.com/ray-project/ray/blob/master/python/ray/train/torch/config.py#L169</a>
    </p>
    <pre>
```python
def set_env_vars(addr, port):
    os.environ["MASTER_ADDR"] = addr
    os.environ["MASTER_PORT"] = str(port)

worker_group.execute(set_env_vars, addr=master_addr, port=master_port)
```</pre>
    <p>If you read the documentation at <a
        href="https://pytorch.org/docs/stable/elastic/run.html#module-torch.distributed.run">https://pytorch.org/docs/stable/elastic/run.html#module-torch.distributed.run</a>
      in the Note side, <b>torchrun is a python console script to the main module torch.distributed.run declared in the
        entry_points configuration in setup.py. It is equivalent to invoking python -m torch.distributed.run.</b> So
      basically `TorchTrainer` is also like `torchrun`, it just help you to set the arguments automatically.</p>
    <p>Now let us run `test-ray.py`,</p>
    <pre>
```bash
python3 test-ray.py
```

```text
(TunerInternal pid=14055) Training started without custom configuration.
(RayTrainWorker pid=2180081, ip=100.92.17.27) Setting up process group for: env:// [rank=0, world_size=4]
(TorchTrainer pid=2179995, ip=100.92.17.27) Started distributed worker processes: 
(TorchTrainer pid=2179995, ip=100.92.17.27) - (ip=100.92.17.27, pid=2180081) world_rank=0, local_rank=0, node_rank=0
(TorchTrainer pid=2179995, ip=100.92.17.27) - (ip=100.92.17.27, pid=2180082) world_rank=1, local_rank=1, node_rank=0
(TorchTrainer pid=2179995, ip=100.92.17.27) - (ip=100.93.25.29, pid=14206) world_rank=2, local_rank=0, node_rank=1
(TorchTrainer pid=2179995, ip=100.92.17.27) - (ip=100.93.25.29, pid=14207) world_rank=3, local_rank=1, node_rank=1
(RayTrainWorker pid=14207) 1 3 4 1
(RayTrainWorker pid=14206) 0 2 4 1
(RayTrainWorker pid=2180081, ip=100.92.17.27) 0 0 4 0
(RayTrainWorker pid=2180082, ip=100.92.17.27) 1 1 4 0
```</pre>
    <p>The important logs,</p>
    <pre>
```text
(TorchTrainer pid=2179995, ip=100.92.17.27) - (ip=100.92.17.27, pid=2180081) world_rank=0, local_rank=0, node_rank=0
(TorchTrainer pid=2179995, ip=100.92.17.27) - (ip=100.92.17.27, pid=2180082) world_rank=1, local_rank=1, node_rank=0
(TorchTrainer pid=2179995, ip=100.92.17.27) - (ip=100.93.25.29, pid=14206) world_rank=2, local_rank=0, node_rank=1
(TorchTrainer pid=2179995, ip=100.92.17.27) - (ip=100.93.25.29, pid=14207) world_rank=3, local_rank=1, node_rank=1
```</pre>
    <p>Here you can clearly see local ranks, node ranks and world ranks.</p>
    <h3>Actual 2D Parallelism</h3>
    <p>Now the actual 2D Parallelism, it is quite simple actually,</p>
    <pre>
```python
import torch
import torch.nn as nn
import torch.distributed as dist
import os
import ray
from ray import train
from ray.train import ScalingConfig
from ray.train.torch import TorchTrainer

class Linear(nn.Module):
    def __init__(self, in_features, out_features, group, ranks):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features

        self.rank = int(os.environ['RANK'])
        self.local_rank = int(os.environ['LOCAL_RANK'])
        self.group = group
        self.ranks = ranks
        self.group_rank = dist.get_group_rank(self.group, self.rank)
        self.world_size = group.size()
        self.device = f'cuda:{self.local_rank}'

        self.local_in_features = in_features 
        self.local_out_features = out_features // self.world_size

        self.linear = nn.Linear(self.local_in_features, self.local_out_features)
    
    def forward(self, x, batch_size, broadcast = True):

        if broadcast:
            if self.group_rank == 0:
                dist.broadcast(x, src=self.ranks[0], group=self.group)
            else:
                x = torch.zeros(batch_size, self.local_in_features, device=self.device)
                dist.broadcast(x, src=self.ranks[0], group=self.group)

        local_output = self.linear(x)
        
        gathered_out = [torch.zeros_like(local_output) for _ in range(self.world_size)]
        
        dist.all_gather(gathered_out, local_output, group = self.group)
        gathered_out = torch.cat(gathered_out, dim=-1)

        print(self.rank, gathered_out.shape)

        return gathered_out

def func():
    rank = int(os.environ['RANK'])

    tp_group1 = dist.new_group([0, 1])
    tp_group2 = dist.new_group([2, 3])
    pp_group = dist.new_group([0, 2])

    batch_size = 32
    input_shape = 50
    output_shape = 4

    if rank in [0, 1]:
        linear1 = Linear(input_shape, input_shape, tp_group1, [0, 1])
        linear1 = linear1.to(linear1.device)
        linear2 = Linear(input_shape, input_shape, tp_group1, [0, 1])
        linear2 = linear2.to(linear2.device)
        linear3 = None
        linear4 = None
    else:
        linear1 = None
        linear2 = None
        linear3 = Linear(input_shape, input_shape, tp_group2, [2, 3])
        linear3 = linear3.to(linear3.device)
        linear4 = Linear(input_shape, output_shape, tp_group2, [2, 3])
        linear4 = linear4.to(linear4.device)

    if rank in [0, 1]:
        if rank == 0:
            input_tensor = torch.randn(batch_size, input_shape, device=linear1.device)
        else:
            input_tensor = None
        
        out1 = linear1(input_tensor, batch_size, broadcast = True)
        out2 = linear2(out1, batch_size, broadcast = False)

        if rank == 0:
            dist.broadcast(out2, src=0, group = pp_group)
    else:
        if rank == 2:
            out2 = torch.zeros(batch_size, input_shape, device=linear3.device)
            dist.broadcast(out2, src=0, group = pp_group)
        else:
            out2 = None
        
        out3 = linear3(out2, batch_size, broadcast = True)
        out4 = linear4(out3, batch_size, broadcast = False)
        print(out4.shape)

def main():
    runtime_env = {
        'env_vars': {
            'NCCL_SOCKET_IFNAME': 'tailscale0',
        }
    }
    ray.init(address="ray://localhost:10001", runtime_env = runtime_env)
    scaling_config = ScalingConfig(
        num_workers=4,
        use_gpu=True,
    )
    
    ray_trainer = TorchTrainer(
        func,
        scaling_config=scaling_config,
    )
    ray_trainer.fit()

if __name__ == "__main__":
    main()
```</pre>
    <p>Save it as `2d-parallelism.py` and run it,</p>
    <pre>
```bash
python3 2d-parallelism.py
```</pre>
    <p>The output,</p>
    <pre>
```bash
(RayTrainWorker pid=2423679) 1 torch.Size([32, 50])
(RayTrainWorker pid=2423679) 1 torch.Size([32, 50])
(RayTrainWorker pid=2423678) 0 torch.Size([32, 50])
(RayTrainWorker pid=2423678) 0 torch.Size([32, 50])
(RayTrainWorker pid=2284881, ip=100.92.17.27) 3 torch.Size([32, 50])
(RayTrainWorker pid=2284881, ip=100.92.17.27) 3 torch.Size([32, 4])
(RayTrainWorker pid=2284881, ip=100.92.17.27) torch.Size([32, 4])
(RayTrainWorker pid=2284880, ip=100.92.17.27) 2 torch.Size([32, 50])
(RayTrainWorker pid=2284880, ip=100.92.17.27) 2 torch.Size([32, 4])
(RayTrainWorker pid=2284880, ip=100.92.17.27) torch.Size([32, 4])
```</pre>
    <p>You can see `3 torch.Size([32, 4])`, which is the last output that we want. So the flow is like,</p>
    <p>- 1. You need to make sure you set `'NCCL_SOCKET_IFNAME': 'tailscale0'`. Because we use Tailscale, we set it
      `tailscale0`, verify using `ifconfig`. This is to let NCCL know which network need
      to use for the communication. You can put multiple networks split by commas.</p>
    <p>- 2. Initialize communication group,</p>
    <p>-- i. `tp_group1 = dist.new_group([0, 1])` between GPU 0 and GPU 1.</p>
    <p>-- ii. `tp_group2 = dist.new_group([2, 3])` between GPU 2 and GPU 3.</p>
    <p>-- iii. `pp_group = dist.new_group([0, 2])` between GPU 0 and GPU 2.</p>
    <p>- 3. Initialize all the layers using If-Else statement, you can do it better to support dynamic layers.</p>
    <p>-- i. `if rank in [0, 1]: linear1 = Linear(input_shape, input_shape, tp_group1, [0, 1])`. GPU 0 and GPU 1 both
      initialized `linear1` with the communication `tp_group1`.</p>
    <p>-- ii. `if rank in [0, 1]: linear2 = Linear(input_shape, input_shape, tp_group1, [0, 1])`. GPU 0 and GPU 1 both
      initialized `linear2` with the communication `tp_group1`.</p>
    <p>-- iii. `if rank in [2, 3]: linear3 = Linear(input_shape, input_shape, tp_group2, [2, 3])`. GPU 2 and GPU 3 both
      initialized `linear3` with the communication `tp_group2`.</p>
    <p>-- iv. `if rank in [0, 1]: linear4 = Linear(input_shape, input_shape, tp_group2, [2, 3])`. GPU 3 and GPU 3 both
      initialized `linear4` with the communication `tp_group2`.</p>
    <p>- 4. `def __init__(self, in_features, out_features, group, ranks)` The reason why we pass the `ranks` is to make
      sure during the broadcast, the broadcaster come from the local group `src`, `dist.broadcast(x, src=self.ranks[0],
      group=self.group)`.</p>
    <p>- 5. `self.group_rank = dist.get_group_rank(self.group, self.rank)` this also to get the ranks based on the
      group, if the group is [2, 3], so the group rank is [0, 1]. When group is rank == 0, we can do broadcast if you
      want.</p>
    <p>- 6. `self.device = f'cuda:{self.local_rank}'`. The reason why `self.device` must use local rank because, as we
      know, we got 2 nodes, each node with 2 GPUs, even though the second GPU and the second node is rank 3, but local
      rank is 1. So you must initialize as `cuda:1` at the second node.</p>
    <p>- 7. We initialized 4 hidden layers, each hidden layer got a linear layer with size 50x50, except for the last
      layer
      is 50x4.
      Because each hidden
      layer been TP,</p>
    <p>-- first layer, GPU 0 50x25 GPU 1 50x25.</p>
    <p>-- second layer, GPU 0 50x25 GPU 1 50x25.</p>
    <p>-- third layer, GPU 2 50x25 GPU 3 50x25.</p>
    <p>-- fourth layer, GPU 2 50x2 GPU 3 50x2.</p>
    <p>- The data flow,</p>
    <p>- 8. the input with size 32x50 will initialize at GPU 0, this will broadcast using `dist.broadcast` to GPU 1
      using TP Group: [0, 1].</p>
    <p>- 9. On the first hidden layer, now GPU 0 input 32x50 matmul 50x25 = 32x25, GPU 1 input 32x50 matmul 50x25 =
      32x25, and do `dist.all_gather`. So GPU 0 and GPU 1 will got the same list of matrices [32x25, 32x25], and GPU 0
      and
      GPU 1 will do concatenation on the last dimension, so it will become 32x50, ready to pass to second hidden layer.
    </p>
    <p>- 10. On the second hidden layer, now GPU 0 input 32x50 matmul 50x25 = 32x25, GPU 1 input 32x50 matmul 50x25 =
      32x25, and do `dist.all_gather`. So GPU 0 and GPU 1 will got the same list of matrices [32x25, 32x25], and GPU 0
      and
      GPU 1 will do concatenation on the last dimension, so it will become 32x50, ready to broadcast to GPU 2 using PP
      Group: [0, 2].</p>
    <p>- 11. GPU 0 will broadcast using `dist.broadcast` to GPU 2 using PP Group: [0, 2], so GPU 2 input is 32x50.</p>
    <p>- 12. GPU 2 will broadcast using `dist.broadcast` to GPU 3 using TP Group: [2, 3], so GPU 3 input is 32x50.</p>
    <p>- 13. On the third hidden layer, now GPU 2 input 32x50 matmul 50x25 = 32x25, GPU 3 input 32x50 matmul 50x25 =
      32x25, and do `dist.all_gather`. So GPU 2 and GPU 3 will got the same list of matrices [32x25, 32x25], and GPU 0
      and
      GPU 1 will do concatenation on the last dimension, so it will become 32x50, ready to pass to fourth hidden layer.
    </p>
    <p>- 14. On the fourth hidden layer, now GPU 2 input 32x50 matmul 50x2 = 32x2, GPU 3 input 32x50 matmul 50x2 =
      32x2, and do `dist.all_gather`. So GPU 2 and GPU 3 will got the same list of matrices [32x2, 32x2], and GPU 2
      and
      GPU 3 will do concatenation on the last dimension, so it will become 32x4, ready to pass back to CPU.</p>

    <p>- 15. The data movement is like below,</p>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1050 1250">
      <!-- Styles -->
      <defs>
        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="#000" />
        </marker>
        <style type="text/css">
          text {
            font-family: Arial, sans-serif;
          }

          .title {
            font-size: 16px;
            font-weight: bold;
          }

          .subtitle {
            font-size: 14px;
          }

          .label {
            font-size: 12px;
          }

          .small {
            font-size: 10px;
          }
        </style>
      </defs>

      <!-- Title -->
      <text x="500" y="30" text-anchor="middle" class="title">TP and PP GPU Communication Diagram inside
        tailscale0</text>

      <!-- GPUs -->
      <g id="gpus">
        <rect x="50" y="50" width="200" height="1150" fill="#e6f3ff" stroke="#0066cc" stroke-width="2" />
        <rect x="300" y="50" width="200" height="1150" fill="#e6f3ff" stroke="#0066cc" stroke-width="2" />
        <rect x="550" y="50" width="200" height="1150" fill="#e6f3ff" stroke="#0066cc" stroke-width="2" />
        <rect x="800" y="50" width="200" height="1150" fill="#e6f3ff" stroke="#0066cc" stroke-width="2" />
        <text x="150" y="80" text-anchor="middle" class="subtitle">GPU 0, 100.93.25.29</text>
        <text x="400" y="80" text-anchor="middle" class="subtitle">GPU 1, 100.93.25.29</text>
        <text x="650" y="80" text-anchor="middle" class="subtitle">GPU 2, 100.92.17.27</text>
        <text x="900" y="80" text-anchor="middle" class="subtitle">GPU 3, 100.92.17.27</text>
      </g>

      <!-- Input Initialization -->
      <g id="input-init">
        <text x="10" y="120" class="subtitle">Input Initialization</text>
        <rect x="75" y="130" width="150" height="40" fill="#ffcc99" stroke="#ff6600" stroke-width="2" />
        <text x="150" y="155" text-anchor="middle" class="label">Input (32x50)</text>
        <line x1="225" y1="150" x2="380" y2="220" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <text x="275" y="140" text-anchor="middle" class="small">dist.broadcast TP Group [0, 1]</text>
      </g>

      <!-- Hidden Layer 1 -->
      <g id="hidden-layer-1">
        <text x="10" y="220" class="subtitle">Hidden Layer 1</text>
        <rect x="75" y="230" width="150" height="40" fill="#ccffcc" stroke="#009900" stroke-width="2" />
        <text x="150" y="255" text-anchor="middle" class="label">Output (32x25)</text>
        <rect x="325" y="230" width="150" height="40" fill="#ccffcc" stroke="#009900" stroke-width="2" />
        <text x="400" y="255" text-anchor="middle" class="label">Output (32x25)</text>
        <line x1="150" y1="270" x2="150" y2="310" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="400" y1="270" x2="400" y2="310" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <text x="275" y="290" text-anchor="middle" class="small">dist.all_gather TP Group [0, 1]</text>
        <rect x="75" y="320" width="400" height="40" fill="#99ccff" stroke="#0066cc" stroke-width="2" />
        <text x="275" y="345" text-anchor="middle" class="label">Concatenated Output (32x50)</text>
      </g>

      <!-- Hidden Layer 2 -->
      <g id="hidden-layer-2">
        <text x="10" y="420" class="subtitle">Hidden Layer 2</text>
        <rect x="75" y="430" width="150" height="40" fill="#ccffcc" stroke="#009900" stroke-width="2" />
        <text x="150" y="455" text-anchor="middle" class="label">Output (32x25)</text>
        <rect x="325" y="430" width="150" height="40" fill="#ccffcc" stroke="#009900" stroke-width="2" />
        <text x="400" y="455" text-anchor="middle" class="label">Output (32x25)</text>
        <line x1="150" y1="470" x2="150" y2="510" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="400" y1="470" x2="400" y2="510" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <text x="275" y="490" text-anchor="middle" class="small">dist.all_gather TP Group [0, 1]</text>
        <rect x="75" y="520" width="400" height="40" fill="#99ccff" stroke="#0066cc" stroke-width="2" />
        <text x="275" y="545" text-anchor="middle" class="label">Concatenated Output (32x50)</text>
        <line x1="275" y1="560" x2="640" y2="625" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <text x="500" y="580" text-anchor="middle" class="small">dist.broadcast PP Group [0, 2]</text>
      </g>

      <!-- Hidden Layer 3 -->
      <g id="hidden-layer-3">
        <text x="510" y="620" class="subtitle">Hidden Layer 3</text>
        <rect x="575" y="630" width="150" height="40" fill="#ffcc99" stroke="#ff6600" stroke-width="2" />
        <text x="650" y="655" text-anchor="middle" class="label">Input (32x50)</text>
        <line x1="725" y1="650" x2="875" y2="720" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <text x="775" y="640" text-anchor="middle" class="small">dist.broadcast TP Group [2, 3]</text>
        <rect x="575" y="730" width="150" height="40" fill="#ccffcc" stroke="#009900" stroke-width="2" />
        <text x="650" y="755" text-anchor="middle" class="label">Output (32x25)</text>
        <rect x="825" y="730" width="150" height="40" fill="#ccffcc" stroke="#009900" stroke-width="2" />
        <text x="900" y="755" text-anchor="middle" class="label">Output (32x25)</text>
        <line x1="650" y1="770" x2="650" y2="810" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="900" y1="770" x2="900" y2="810" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <text x="775" y="790" text-anchor="middle" class="small">dist.all_gather TP Group [2, 3]</text>
        <rect x="575" y="820" width="400" height="40" fill="#99ccff" stroke="#0066cc" stroke-width="2" />
        <text x="775" y="845" text-anchor="middle" class="label">Concatenated Output (32x50)</text>
      </g>

      <!-- Hidden Layer 4 -->
      <g id="hidden-layer-4">
        <text x="510" y="920" class="subtitle">Hidden Layer 4</text>
        <rect x="575" y="930" width="150" height="40" fill="#ccffcc" stroke="#009900" stroke-width="2" />
        <text x="650" y="955" text-anchor="middle" class="label">Output (32x2)</text>
        <rect x="825" y="930" width="150" height="40" fill="#ccffcc" stroke="#009900" stroke-width="2" />
        <text x="900" y="955" text-anchor="middle" class="label">Output (32x2)</text>
        <line x1="650" y1="970" x2="650" y2="1010" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="900" y1="970" x2="900" y2="1010" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <text x="775" y="990" text-anchor="middle" class="small">dist.all_gather TP Group [2, 3]</text>
        <rect x="575" y="1020" width="400" height="40" fill="#99ccff" stroke="#0066cc" stroke-width="2" />
        <text x="775" y="1045" text-anchor="middle" class="label">Concatenated Output (32x4)</text>
      </g>

      <!-- Final Output -->
      <g id="final-output">
        <text x="510" y="1120" class="subtitle">Final Output</text>
        <rect x="675" y="1130" width="200" height="40" fill="#ffff99" stroke="#999900" stroke-width="2" />
        <text x="775" y="1155" text-anchor="middle" class="label">Output (32x4)</text>
        <text x="775" y="1180" text-anchor="middle" class="small">Ready to pass back to CPU</text>
      </g>
    </svg>
    <p>Super cool right?</p>
    <p>---</p>
    <p>thats all, give some love to <a href="https://x.com/aisyahhhrzk" target="_blank">Aisyah Razak.</a></p>
  </div>
</body>

</html>