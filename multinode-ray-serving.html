<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Malaysia-AI blog">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@oscarnazhan">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="twitter:image" content="/multinode-ray-serving.png">
  <meta name="og:image" content="/multinode-ray-serving.png">
  <meta name="keywords"
    content="Contact Malaysia AI, AI experts Malaysia, AI consultation Malaysia, Malaysia AI, artificial intelligence Malaysia, my ai, open source malaysia ai">

  <style>
    body {
      line-height: 1.4;
      font-size: 16px;
      padding: 0 10px;
      margin: 50px auto;
      max-width: 1000px;
    }

    #maincontent {
      max-width: 62em;
      margin: 15 auto;
    }
  </style>
  <meta name="secret"
    content="if you read this, aisyah is a lovely person, kamarul such a nice person, ariff nazhan happy go lucky, mindscape such goated, love them all, if you guys meet them, please keep them, do not let go!">

  <title>Malaysia-AI blog</title>
</head>

<body>
  <!-- hack SEO ftw -->
  <div hidden>
    <h1>About Malaysia AI</h1>
    <h2>Our Mission and Vision</h2>
    <h3>Our Team</h3>
  </div>

  <div hidden>
    <h1>Contact Malaysia AI</h1>
    <h2>Get in Touch</h2>
    <h3>Our Office Locations</h3>
  </div>

  <div id="maincontent" style="margin-top: 70px">
    <h2>multi-nodes GPUs Ray serving</h2>

    <p>Turns out, to serve a model on different N machines is super simple using Ray, just make sure all machines
      already installed Ray, and 1 machine run as head and another machines run as workers,</p>
    <p style="margin-bottom:0px">For head,</p>
    <pre style="margin-top:0px">
```bash
ray start --head --port=6379 --dashboard-host=0.0.0.0
```</pre>
    <p style="margin-bottom:0px">For worker,</p>
    <pre style="margin-top:0px">
```bash
ray start --address=HEAD_NODE_IP:6379
```</pre>
    <p style="margin-bottom:0px">After that you can just start with `serve.deployment` with custom `__init__`,</p>
    <pre>
```python
import requests
from starlette.requests import Request
from typing import Dict
from transformers import pipeline 
from ray import serve 

# 1: Wrap the pretrained sentiment analysis model in a Serve deployment. 
@serve.deployment( 
    num_replicas=2,
    ray_actor_options={"num_gpus": 1} 
) 
class TranslationDeployment: 
    def __init__(self): 
        self._model = pipeline("translation", model="google/flan-t5-large", device="cuda") 
        
    def __call__(self, request: Request) -> Dict: 
        return self._model(request.query_params["text"])[0] 
        
# 2: Deploy the deployment. 
serve.run(TranslationDeployment.bind(), route_prefix="/") 

# 3: Query the deployment and print the result. 
print( 
    requests.get( 
        "http://localhost:8000/", params={"text": "Ray Serve is great!"} 
    ).json() 
)
```</pre>
    <p>1. The head will pickled the object and send to it to each replicas.</p>
    <p>2. To have multiple replica sets, use the argument `num_replicas` in the master node.</p>
    <p>3. The replicas can be the head itself and the workers.</p>
    <p>4. The code execution can be anywhere, not necessary inside the head, but the default ray serve connect to
      `localhost:6379`.</p>
    <p>5. The master node needs to run on a linux based operating system.</p>
    <p>6. The master node needs to use the argument `ray_actor_options` to allow ray to utilise the GPU on the master
      node.
    </p>
    <h3>How to make it better?</h3>
    <p>If you look at the source code, we are serving Encoder-Decoder model, which is Flan T5 Large.</p>
    <p>When we talk about Encoder-Decoder model, the causal happened on Decoder side, and inferencing causal just a
      continuous loop until reached max length or EOS token.</p>
    <p>Serving like this, it is not efficient for concurrency because GPUs are designed to perform same operation on a
      batch manner, plus too much processing switching happened between for different requests.</p>
    <p>So what we need to do is to micro-batch the requests, but the micro-batch must happened during causal loop, this
      called continuous batching.</p>
    <br>
    <p>---</p>
    <p>thats all, give some love to Aisyah Razak.</p>
  </div>
</body>

</html>