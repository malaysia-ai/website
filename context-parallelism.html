<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- HTML Meta Tags -->
  <title>Malaysia-AI blog Context Parallelism using Ray PyTorch</title>
  <meta name="description" content="Malaysia-AI blog Context Parallelism">

  <!-- Facebook Meta Tags -->
  <meta property="og:url" content="https://malaysia-ai.org/context-parallelism">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Malaysia-AI blog Context Parallelism using Ray PyTorch">
  <meta property="og:description" content="Malaysia-AI blog Context Parallelism using Ray PyTorch">
  <meta property="og:image" content="https://malaysia-ai.org/context-parallelism.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta property="twitter:domain" content="malaysia-ai.org">
  <meta property="twitter:url" content="https://malaysia-ai.org/context-parallelism">
  <meta name="twitter:title" content="Malaysia-AI blog Context Parallelism using Ray PyTorch">
  <meta name="twitter:description" content="Malaysia-AI blog Context Parallelism">
  <meta name="twitter:image" content="https://malaysia-ai.org/context-parallelism.png">

  <!-- Meta Tags Generated via https://www.opengraph.xyz -->

  <style>
    body {
      line-height: 1.4;
      font-size: 16px;
      padding: 0 10px;
      margin: 50px auto;
      max-width: 1000px;
    }

    #maincontent {
      max-width: 62em;
      margin: 15 auto;
    }

    pre {
      margin-top: 0px;
      white-space: break-spaces;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
      options: {
        enableMenu: false
      }
    };
  </script>
</head>

<body>

  <div id="maincontent" style="margin-top: 70px">
    <h2>Context Parallelism using Ray PyTorch</h2>
    <p>As you can see, Large Language Model is taking over the world, everyone is using it, and it able to augment
      humanity productivity and intelligence beyond what we expect.</p>
    <p>You can chat with the LLM to do practically everything you want, from roleplaying as a baby up to asking feedback
      loops for research papers that you do not understand.</p>
    <p>During ChatGPT released on November 30, 2022, it only support max 4096 context length or 4096 tokens, 1 token
      average 2 words based on ChatGPT tokenizer, so 8192 words. Let use chat bubbles below as an example, green chat
      bubbles is the user while
      gray chat bubbles is the assistant,</p>
    <svg width="450" height="250" xmlns="http://www.w3.org/2000/svg">
      <!-- User bubble -->
      <rect x="10" y="20" width="160" height="30" rx="10" ry="10" fill="#DCF8C6" />
      <text x="20" y="40" font-family="Arial" font-size="14" fill="#000">hello</text>

      <!-- Assistant bubble -->
      <rect x="230" y="80" width="200" height="30" rx="10" ry="10" fill="#ECECEC" />
      <text x="240" y="100" font-family="Arial" font-size="14" fill="#000">Hi! How can I help you?</text>

      <!-- User bubble -->
      <rect x="10" y="140" width="190" height="30" rx="10" ry="10" fill="#DCF8C6" />
      <text x="20" y="160" font-family="Arial" font-size="14" fill="#000">do u know about Malaysia?</text>

      <!-- Assistant bubble -->
      <rect x="230" y="200" width="200" height="30" rx="10" ry="10" fill="#ECECEC" />
      <text x="240" y="220" font-family="Arial" font-size="14" fill="#000">Of course I know Malaysia!</text>
    </svg>
    <p>For this example, let us assume 1 token equal to 1 word, so the words are ['hello', 'hi!', 'How', 'can', 'I',
      'help', 'you?', 'do', 'u', 'know', 'about', 'Malaysia?', 'Of', 'course', 'I', 'know', 'about', 'Malaysia!'], 18
      words or 18 tokens. So when when say the LLM support 4096 context length, it can support multi-turn conversation
      will the total 4096 tokens.</p>
    <p>Today, LLM can support million tokens of context length, Gemini from Google can support up to 1 million tokens of
      context length, you can give an entire book or research paper and ask any question that you want!</p>
    <svg width="450" height="250" xmlns="http://www.w3.org/2000/svg">
      <!-- User bubble -->
      <rect x="10" y="20" width="160" height="30" rx="10" ry="10" fill="#DCF8C6" />
      <text x="20" y="40" font-family="Arial" font-size="14" fill="#000">hello</text>

      <!-- Assistant bubble -->
      <rect x="230" y="80" width="200" height="30" rx="10" ry="10" fill="#ECECEC" />
      <text x="240" y="100" font-family="Arial" font-size="14" fill="#000">Hi! How can I help you?</text>

      <!-- User bubble -->
      <rect x="10" y="140" width="320" height="30" rx="10" ry="10" fill="#DCF8C6" />
      <text x="20" y="160" font-family="Arial" font-size="14" fill="#000">based on this paper bla bla .., what is bla
        bla ..?</text>

      <!-- Assistant bubble -->
      <rect x="110" y="200" width="320" height="30" rx="10" ry="10" fill="#ECECEC" />
      <text x="120" y="220" font-family="Arial" font-size="14" fill="#000">Based on the page 3, the answer is bla bla
        ..</text>
    </svg>
    <p><b>From 4096 context length up to 1 million context length in less than 2 years!</b></p>
    <p>How does LLM able to understand from just 4096 tokens to become 1 million tokens? Context Parallelism!</a>But
      before that, we need to know the memory usage of Attention mechanism,</p>

    <h3>Attention is all you need!</h3>
    <p>And Attention mechanism defined as,</p>

    <div>
      $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$
    </div>
    <p>Where `Q` is query matrix, `K` is key matrix, and `V` is value matrix. LLM is decoder model so the attention
      happened is self-attention. Now for an example,
      ,
    </p>
    <p>- Hidden size or `d_model` for QKV is 10, so QKV with each size [2, 10], 2 input dimension, 10 hidden dimension.
    </p>
    <p>- the input shape is [5, 2], 5 sequence length or `L`, 2 hidden dimension or `in_d_model`.</p>
    <p>- Input will matmul with
      QKV matrices,</p>
    <p>- 1. input [5,2] matmul Q [2, 10] = [5, 10]</p>
    <p>- 2. input [5,2] matmul K [2, 10] = [5, 10]</p>
    <p>- 3. input [5,2] matmul V [2, 10] = [5, 10]</p>
    <p>- 4. After that calculate Attention,</p>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 400">
      <!-- Q matrix -->
      <rect x="50" y="50" width="100" height="50" fill="#FFB3BA" stroke="black" />
      <text x="100" y="80" text-anchor="middle" dominant-baseline="middle" font-size="10">Q [5x10]</text>

      <!-- K^T matrix -->
      <rect x="200" y="50" width="50" height="100" fill="#BAFFC9" stroke="black" />
      <text x="225" y="100" text-anchor="middle" dominant-baseline="middle" font-size="10">K^T [10x5]</text>

      <!-- QK^T result -->
      <rect x="300" y="50" width="50" height="50" fill="#BAE1FF" stroke="black" />
      <text x="325" y="80" text-anchor="middle" dominant-baseline="middle" font-size="10">QK^T [5x5]</text>

      <!-- Softmax -->
      <rect x="400" y="50" width="50" height="50" fill="#FFDFBA" stroke="black" />
      <text x="425" y="80" text-anchor="middle" dominant-baseline="middle" font-size="10">Softmax</text>

      <!-- V matrix -->
      <rect x="500" y="50" width="100" height="50" fill="#FFFFBA" stroke="black" />
      <text x="550" y="80" text-anchor="middle" dominant-baseline="middle" font-size="10">V [5x10]</text>

      <!-- Final result -->
      <rect x="650" y="50" width="100" height="50" fill="#E6BAFF" stroke="black" />
      <text x="700" y="80" text-anchor="middle" dominant-baseline="middle" font-size="10">Result [5x10]</text>

      <!-- Arrows -->
      <line x1="150" y1="75" x2="190" y2="75" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="250" y1="75" x2="290" y2="75" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="350" y1="75" x2="390" y2="75" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="450" y1="75" x2="490" y2="75" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="600" y1="75" x2="640" y2="75" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

      <!-- Arrowhead definition -->
      <defs>
        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
          <polygon points="0 0, 5 3.5, 0 7" />
        </marker>
      </defs>

      <!-- Labels -->
      <text x="175" y="60" text-anchor="middle" font-size="14">matmul</text>
      <text x="275" y="60" text-anchor="middle" font-size="14">matmul</text>
      <text x="375" y="60" text-anchor="middle" font-size="14">apply</text>
      <text x="475" y="60" text-anchor="middle" font-size="14">matmul</text>
      <text x="625" y="60" text-anchor="middle" font-size="14">matmul</text>

      <!-- Legend -->
      <text x="50" y="200" font-size="16" font-weight="bold">Legend:</text>
      <rect x="50" y="220" width="20" height="20" fill="#FFB3BA" stroke="black" />
      <text x="80" y="235" font-size="14">Q matrix</text>
      <rect x="50" y="250" width="20" height="20" fill="#BAFFC9" stroke="black" />
      <text x="80" y="265" font-size="14">K^T matrix</text>
      <rect x="50" y="280" width="20" height="20" fill="#BAE1FF" stroke="black" />
      <text x="80" y="295" font-size="14">QK^T result</text>
      <rect x="50" y="310" width="20" height="20" fill="#FFDFBA" stroke="black" />
      <text x="80" y="325" font-size="14">Softmax</text>
      <rect x="50" y="340" width="20" height="20" fill="#FFFFBA" stroke="black" />
      <text x="80" y="355" font-size="14">V matrix</text>
      <rect x="50" y="370" width="20" height="20" fill="#E6BAFF" stroke="black" />
      <text x="80" y="385" font-size="14">Final result</text>
    </svg>
    <p>The output shape should be [Q `L`, V `d_model`] = [5, 10]. To calculate the memory usage roughly <b>based on
        output
        shape</b>,</p>
    <p>- 1. Q, K and V linear weights, which each output is [in_d_model, d_model], <b>3 x in_d_model x d_model</b>.</p>
    <p>- 2. input matmul Q, K and V, which each output is [L, d_model], <b>3 x L x d_model</b>.</p>
    <p>- 3. softmax(QK^T)V, [L, d_model], <b>L x d_model</b>.</p>
    <p>- 4. Total, (3 x in_d_model x d_model) + (3 x L x d_model) + (L x d_model) = 260.</p>
    <p>- 7. Assumed we store in bfloat16, 260 x 2 = <b>520 bytes</b>.</p>

    <p>520 bytes is super small and yes that is for a simple example, but what if we use at least LLM 8B parameters such
      as Llama 3.1?</p>
    <h3>Use actual Llama 3.1 8B parameters</h3>
    <p>Based on the Llama 3.1 8B parameters settings from HuggingFace, <a
        href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/config.json">https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/config.json</a>,
      there are 3 settings important for attention size,
    <p>- 1. `hidden_size` = 4096.</p>
    <p>- 2. `num_attention_heads` = 32.</p>
    <p>Because Llama use multi-head attention and <b>to simplify the attention, assumed no group multi-head attention
        been used aka
        `num_key_value_heads`</b>, assume the input shape is [5, 4096], 5 sequence length with 4096 hidden size, so
      during calculating the attention,</p>
    <p>- 1. `head_dim` = hidden_size // num_attention_heads</p>
    <p>- 2. Q, K, V linear weights [hidden_size, num_attention_heads x head_dim], <b>3 x hidden_size x
        num_attention_heads x head_dim</b>.
    </p>
    <p>- 3. input matmul Q, K and V, which each output is [L, num_attention_heads x head_dim] and reshape become
      [num_attention_heads, L,
      head_dim], <b>3 x L x num_attention_heads x head_dim</b>.</p>
    <p>- 4. softmax(QK^T)V = [num_attention_heads, L, head_dim], <b>num_attention_heads x L x head_dim</b>.</p>
    <p>- 5. Total, (3 x hidden_size x num_attention_heads x head_dim) + (3 x L x num_attention_heads x head_dim) +
      (num_attention_heads x L x head_dim) = 50413568.
    </p>
    <p>- 6. Assumed we store in bfloat16, 50413568 x 2 = <b>100827136 bytes or 0.100827136 GB</b>, still small.</p>
    <p>Now what if you got 1M sequence length or 1M context length? replace the `L` with 1M, you got 16434331648
      bytes, saved as bfloat16, <b>16434331648 x 2 = 32868663296 bytes or 32.868663296 GB!</b></p>
    <p><b>32.868663296 GB</b> just for the attention, not included other linear layers and other matmul operations,
      insane. How about 13B or 70B parameters? kebabom!</p>
    <h3>Context Parallelism</h3>
    <p>When we talk about Parallelism in deep learning, it is about how to parallelize the data into multiple GPUs
      either to reduce computation burden and at the same reduce memory consumption or replicating the replica to
      increase the
      size of input to make learning process faster, and Context Parallelism is about
      how to parallelize the sequence length into multiple GPUs.</p>
    <div style="width: 60%">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 600 1000">
        <!-- Original Matrix -->
        <rect x="200" y="20" width="200" height="80" fill="#A0D8EF" stroke="black" />
        <text x="300" y="60" text-anchor="middle" font-size="14">Original Matrix</text>
        <text x="300" y="80" text-anchor="middle" font-size="12">[1, 1000000, 512]</text>

        <!-- Arrow -->
        <line x1="300" y1="100" x2="200" y2="150" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="300" y1="100" x2="400" y2="150" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Split Matrices -->
        <rect x="50" y="170" width="150" height="70" fill="#FFD700" stroke="black" />
        <text x="125" y="200" text-anchor="middle" font-size="14">GPU 1</text>
        <text x="125" y="220" text-anchor="middle" font-size="12">[1, 500000, 512]</text>

        <rect x="400" y="170" width="150" height="70" fill="#98FB98" stroke="black" />
        <text x="475" y="200" text-anchor="middle" font-size="14">GPU 2</text>
        <text x="475" y="220" text-anchor="middle" font-size="12">[1, 500000, 512]</text>

        <!-- Arrows to Local Attention -->
        <line x1="125" y1="240" x2="125" y2="290" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="475" y1="240" x2="475" y2="290" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Local Attention -->
        <rect x="50" y="310" width="150" height="70" fill="#FFD700" stroke="black" />
        <text x="125" y="340" text-anchor="middle" font-size="14">Local Attention</text>
        <text x="125" y="360" text-anchor="middle" font-size="12">Calculation</text>

        <rect x="400" y="310" width="150" height="70" fill="#98FB98" stroke="black" />
        <text x="475" y="340" text-anchor="middle" font-size="14">Local Attention</text>
        <text x="475" y="360" text-anchor="middle" font-size="12">Calculation</text>

        <!-- Arrows to Linear Layer -->
        <line x1="125" y1="380" x2="125" y2="430" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="475" y1="380" x2="475" y2="430" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Linear Layer -->
        <rect x="50" y="450" width="150" height="70" fill="#DDA0DD" stroke="black" />
        <text x="125" y="480" text-anchor="middle" font-size="14">Linear Layer</text>
        <text x="125" y="500" text-anchor="middle" font-size="12">Output Logits</text>

        <rect x="400" y="450" width="150" height="70" fill="#DDA0DD" stroke="black" />
        <text x="475" y="480" text-anchor="middle" font-size="14">Linear Layer</text>
        <text x="475" y="500" text-anchor="middle" font-size="12">Output Logits</text>

        <!-- Arrows to Loss Calculation -->
        <line x1="125" y1="520" x2="125" y2="570" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="475" y1="520" x2="475" y2="570" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Loss Calculation -->
        <rect x="50" y="590" width="150" height="70" fill="#F08080" stroke="black" />
        <text x="125" y="620" text-anchor="middle" font-size="14">Loss Calculation</text>
        <text x="125" y="640" text-anchor="middle" font-size="12">GPU 1 Loss</text>

        <rect x="400" y="590" width="150" height="70" fill="#F08080" stroke="black" />
        <text x="475" y="620" text-anchor="middle" font-size="14">Loss Calculation</text>
        <text x="475" y="640" text-anchor="middle" font-size="12">GPU 2 Loss</text>

        <!-- Arrows to Loss Gathering -->
        <line x1="125" y1="660" x2="125" y2="740" stroke="black" stroke-width="2" />
        <line x1="475" y1="660" x2="475" y2="740" stroke="black" stroke-width="2" />
        <line x1="125" y1="740" x2="300" y2="740" stroke="black" stroke-width="2" />
        <line x1="475" y1="740" x2="300" y2="740" stroke="black" stroke-width="2" />
        <line x1="300" y1="740" x2="300" y2="780" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Loss Gathering -->
        <rect x="200" y="800" width="200" height="80" fill="#90EE90" stroke="black" />
        <text x="300" y="830" text-anchor="middle" font-size="14">Gather Losses</text>
        <text x="300" y="850" text-anchor="middle" font-size="12">Average Loss</text>

        <!-- Labels -->
        <text x="300" y="920" text-anchor="middle" font-size="16" font-weight="bold">Context Parallelism</text>
        <text x="300" y="940" text-anchor="middle" font-size="14">Separate Logits and Loss</text>
        <text x="300" y="960" text-anchor="middle" font-size="14">Calculation per GPU</text>

        <!-- Arrowhead definition -->
        <defs>
          <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
            <polygon points="0 0, 10 3.5, 0 7" />
          </marker>
        </defs>
      </svg>
    </div>
    <p>So now each GPUs can calculate their own local attention but still coherent with the other local attentions and
      if you gather and combine the local attentions, the combined should be almost the same with the full attention
      with super super small different.</p>

    <p>If we split the QKV into 2 GPUs, Q = [Q1, Q2], K = [K1, K2], V = [V1, V2], so local attentions,
      Attention1=softmax(Q1K1^T)V1
      and
      Attention2=softmax(Q2K2^T)V2.</p>
    <p><b>Now, how does softmax(Q1K1^T)V1 able to correlate with softmax(Q2K2^T)V2 ?</b> Especially on softmax, because
      softmax required sum of exponents on the hidden dimension.</p>

    <h3>Blockwise Parallel Transformer
      for Large Context Models</h3>
    <p>This paper <a href="https://arxiv.org/pdf/2305.19370">https://arxiv.org/pdf/2305.19370</a> shows that we can
      calculate Attention in partition manner, and when we talk about partition manner, the partitions can be
      incrementally
      load from RAM / disk offload by simply loop the partitions and pass to GPU to do the calculation, or split the
      partitions into multi GPUs.</p>
    <p>This Attention in partition manner or blocks called Blockwise Attention by the paper.</p>
    <p>In section 3, it stated Q can split into Bq blocks, and KV split into Bkv blocks.</p>
    <p>For each query block, the blockwise attention Attention(Qi, Kj, Vj) can be computed by iterating over all
      key-value
      blocks,</p>

    <div>
      $$
      \mathrm{Attention}(Q_i, K, V) = \mathrm{Scaling}(\{\exp(Q_i K_j^T)V_j\}_{j=1}^{B_{kv}})
      $$
    </div>
    <p>The scaling operation scales each blockwise attention based on the difference between the blockwise
      maximum and the global maximum.</p>
    <div>
      $$
      \mathrm{Attention}(Q_i, K_j, V_j) = \exp\bigl(Q_i K_j^T - \max(Q_i K_j^T)\bigr) / \sum \exp\bigl(Q_i K_j^T -
      \max(Q_i K_j^T)\bigr)
      $$
    </div>
    <div>
      $$
      \mathrm{max}_i = \max \bigl(\max(Q_i K_1^T), \ldots, \max(Q_i K_B^T)\bigr)
      $$
    </div>

    <p>Once the
      blockwise attention is computed, the global attention matrix can be obtained by scaling the blockwise
      attention using the difference between the blockwise and global softmax normalization constants.</p>
    <div>
      $$
      \mathrm{Attention}(Q_i, K, V) = \bigl[ \exp(Q_i K_j^T - \mathrm{max}_i)~\mathrm{Attention}(Q_i, K_j, V_j)
      \bigr]_{j=1}^{B_{kv}}.
      $$
    </div>
    <p>Visualization to get for Attention(Qi, K, V),</p>
    <div style="width:80%">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 600">
        <!-- Background -->
        <rect width="800" height="600" fill="#fff" />

        <!-- Q block -->
        <rect x="50" y="50" width="100" height="100" fill="#ff9999" stroke="#000000" stroke-width="2" />
        <text x="100" y="105" text-anchor="middle" font-family="Arial, sans-serif" font-size="16">Qi</text>

        <!-- K-V blocks -->
        <rect x="250" y="50" width="100" height="100" fill="#99ff99" stroke="#000000" stroke-width="2" />
        <rect x="360" y="50" width="100" height="100" fill="#9999ff" stroke="#000000" stroke-width="2" />
        <text x="300" y="105" text-anchor="middle" font-family="Arial, sans-serif" font-size="16">K1</text>
        <text x="410" y="105" text-anchor="middle" font-family="Arial, sans-serif" font-size="16">V1</text>

        <rect x="250" y="160" width="100" height="100" fill="#99ff99" stroke="#000000" stroke-width="2" />
        <rect x="360" y="160" width="100" height="100" fill="#9999ff" stroke="#000000" stroke-width="2" />
        <text x="300" y="215" text-anchor="middle" font-family="Arial, sans-serif" font-size="16">K2</text>
        <text x="410" y="215" text-anchor="middle" font-family="Arial, sans-serif" font-size="16">V2</text>

        <!-- Attention computation -->
        <rect x="500" y="50" width="120" height="100" fill="#ffcc00" stroke="#000000" stroke-width="2" />
        <text x="560" y="105" text-anchor="middle" font-family="Arial, sans-serif"
          font-size="14">Attention(Qi,K1,V1)</text>

        <rect x="500" y="160" width="120" height="100" fill="#ffcc00" stroke="#000000" stroke-width="2" />
        <text x="560" y="215" text-anchor="middle" font-family="Arial, sans-serif"
          font-size="14">Attention(Qi,K2,V2)</text>

        <!-- Max operations -->
        <rect x="650" y="50" width="100" height="100" fill="#ff00ff" stroke="#000000" stroke-width="2" />
        <text x="700" y="105" text-anchor="middle" font-family="Arial, sans-serif" font-size="14">max(Qi,K1)</text>

        <rect x="650" y="160" width="100" height="100" fill="#ff00ff" stroke="#000000" stroke-width="2" />
        <text x="700" y="215" text-anchor="middle" font-family="Arial, sans-serif" font-size="14">max(Qi,K2)</text>

        <!-- Global max -->
        <rect x="650" y="300" width="100" height="100" fill="#ff8000" stroke="#000000" stroke-width="2" />
        <text x="700" y="350" text-anchor="middle" font-family="Arial, sans-serif" font-size="14">global_max</text>

        <!-- Final scaled attention -->
        <rect x="350" y="450" width="200" height="100" fill="#00ffff" stroke="#000000" stroke-width="2" />
        <text x="450" y="500" text-anchor="middle" font-family="Arial, sans-serif" font-size="14">Scaled
          Attention(Qi,K,V)</text>

        <!-- Arrows -->
        <defs>
          <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
            <polygon points="0 0, 10 3.5, 0 7" />
          </marker>
        </defs>

        <!-- Qi to K1,V1 and K2,V2 -->
        <line x1="150" y1="100" x2="240" y2="100" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="150" y1="100" x2="240" y2="210" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- K1,V1 and K2,V2 to Attention computations -->
        <line x1="460" y1="100" x2="490" y2="100" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="460" y1="210" x2="490" y2="210" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Attention computations to max operations -->
        <line x1="620" y1="100" x2="640" y2="100" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="620" y1="210" x2="640" y2="210" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Max operations to global max -->
        <line x1="700" y1="150" x2="700" y2="290" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="700" y1="260" x2="700" y2="290" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Attention computations and global max to final scaled attention -->
        <line x1="560" y1="150" x2="450" y2="440" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="560" y1="260" x2="450" y2="440" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="700" y1="400" x2="470" y2="445" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

      </svg>
      <p>- 1. For Qi, loop for each KV blocks, j,</p>
      <p>-- i. Calculate maxj = max(QiKj^T) and Attentionj = softmax(QiKj^T - maxj)Vj</p>
      <p>-- ii. Store maxj in maxi, maxi = [maxj0, maxj1, ...] and Attentionj in Attentioni, Attentioni = [Attentionj0,
        Attentionj1, ...]</p>
      <p>- 2. Calculate max_maxi = max(maxi), loop again KV blocks, j,</p>
      <p>-- i. Calculate blockwisej = exp(QiKj^T - max_maxi) Attentionij, and store it in blockwisei, blockwisei =
        [blockwisej0, blockwisej1, ...]</p>
      <p>-- ii. Sum, now you got Attention(Qi,K,V)!</p>
    </div>

    <p>---</p>
    <p>thats all, give some love to <a href="https://x.com/aisyahhhrzk" target="_blank">Aisyah Razak.</a></p>
  </div>
</body>

</html>