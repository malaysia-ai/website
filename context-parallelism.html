<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- HTML Meta Tags -->
  <title>Malaysia-AI blog Context Parallelism</title>
  <meta name="description" content="Malaysia-AI blog Context Parallelism">

  <!-- Facebook Meta Tags -->
  <meta property="og:url" content="https://malaysia-ai.org/context-parallelism">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Malaysia-AI blog Context Parallelism">
  <meta property="og:description" content="Malaysia-AI blog Context Parallelism">
  <meta property="og:image" content="https://malaysia-ai.org/context-parallelism.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta property="twitter:domain" content="malaysia-ai.org">
  <meta property="twitter:url" content="https://malaysia-ai.org/context-parallelism">
  <meta name="twitter:title" content="Malaysia-AI blog Context Parallelismh">
  <meta name="twitter:description" content="Malaysia-AI blog Context Parallelism">
  <meta name="twitter:image" content="https://malaysia-ai.org/context-parallelism.png">

  <!-- Meta Tags Generated via https://www.opengraph.xyz -->

  <style>
    body {
      line-height: 1.4;
      font-size: 16px;
      padding: 0 10px;
      margin: 50px auto;
      max-width: 1000px;
    }

    #maincontent {
      max-width: 62em;
      margin: 15 auto;
    }

    pre {
      margin-top: 0px;
      white-space: break-spaces;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
      options: {
        enableMenu: false
      }
    };
  </script>
</head>

<body>

  <div id="maincontent" style="margin-top: 70px">
    <h2>Context Parallelism</h2>
    <p>As you can see, Large Language Model is taking over the world, everyone is using it, and it able to augment
      humanity productivity and intelligence beyond what we expect.</p>
    <p>You can chat with the LLM to do practically everything you want, from roleplaying as a baby up to asking feedback
      loops for research papers that you do not understand.</p>
    <p>During ChatGPT released on November 30, 2022, it only support max 4096 context length or 4096 tokens, 1 token
      average 2 words based on ChatGPT tokenizer, so 8192 words. Let use chat bubbles below as an example, green chat
      bubbles is the user while
      gray chat bubbles is the assistant,</p>
    <svg width="450" height="250" xmlns="http://www.w3.org/2000/svg">
      <!-- User bubble -->
      <rect x="10" y="20" width="160" height="30" rx="10" ry="10" fill="#DCF8C6" />
      <text x="20" y="40" font-family="Arial" font-size="14" fill="#000">hello</text>

      <!-- Assistant bubble -->
      <rect x="230" y="80" width="200" height="30" rx="10" ry="10" fill="#ECECEC" />
      <text x="240" y="100" font-family="Arial" font-size="14" fill="#000">Hi! How can I help you?</text>

      <!-- User bubble -->
      <rect x="10" y="140" width="190" height="30" rx="10" ry="10" fill="#DCF8C6" />
      <text x="20" y="160" font-family="Arial" font-size="14" fill="#000">do u know about Malaysia?</text>

      <!-- Assistant bubble -->
      <rect x="230" y="200" width="200" height="30" rx="10" ry="10" fill="#ECECEC" />
      <text x="240" y="220" font-family="Arial" font-size="14" fill="#000">Of course I know Malaysia!</text>
    </svg>
    <p>For this example, let us assume 1 token equal to 1 word, so the words are ['hello', 'hi!', 'How', 'can', 'I',
      'help', 'you?', 'do', 'u', 'know', 'about', 'Malaysia?', 'Of', 'course', 'I', 'know', 'about', 'Malaysia!'], 18
      words or 18 tokens. So when we say the LLM support 4096 context length, it can support multi-turn conversation
      will the total 4096 tokens.</p>
    <p>Today, LLM can support million tokens of context length, Gemini from Google can support up to 1 million tokens of
      context length, you can give an entire book or research paper and ask any question that you want!</p>
    <svg width="450" height="250" xmlns="http://www.w3.org/2000/svg">
      <!-- User bubble -->
      <rect x="10" y="20" width="160" height="30" rx="10" ry="10" fill="#DCF8C6" />
      <text x="20" y="40" font-family="Arial" font-size="14" fill="#000">hello</text>

      <!-- Assistant bubble -->
      <rect x="230" y="80" width="200" height="30" rx="10" ry="10" fill="#ECECEC" />
      <text x="240" y="100" font-family="Arial" font-size="14" fill="#000">Hi! How can I help you?</text>

      <!-- User bubble -->
      <rect x="10" y="140" width="320" height="30" rx="10" ry="10" fill="#DCF8C6" />
      <text x="20" y="160" font-family="Arial" font-size="14" fill="#000">based on this paper bla bla .., what is bla
        bla ..?</text>

      <!-- Assistant bubble -->
      <rect x="110" y="200" width="320" height="30" rx="10" ry="10" fill="#ECECEC" />
      <text x="120" y="220" font-family="Arial" font-size="14" fill="#000">Based on the page 3, the answer is bla bla
        ..</text>
    </svg>
    <p><b>From 4096 context length up to 1 million context length in less than 2 years!</b></p>
    <p>How does LLM able to understand from just 4096 tokens to become 1 million tokens? Context Parallelism!</a>But
      before that, we need to know the memory usage of Attention mechanism,</p>

    <h3>Attention is all you need!</h3>
    <p>And Attention mechanism defined as,</p>

    <div>
      $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$
    </div>
    <p>Where `Q` is query matrix, `K` is key matrix, and `V` is value matrix. LLM is decoder model so the attention
      happened is self-attention. Now for an example,
      ,
    </p>
    <p>- Hidden size or `d_model` for QKV is 10, so QKV with each size [2, 10], 2 input dimension, 10 hidden dimension.
    </p>
    <p>- the input shape is [5, 2], 5 sequence length or `L`, 2 hidden dimension or `in_d_model`.</p>
    <p>- Input will matmul with
      QKV matrices,</p>
    <p>- 1. input [5,2] matmul Q [2, 10] = [5, 10]</p>
    <p>- 2. input [5,2] matmul K [2, 10] = [5, 10]</p>
    <p>- 3. input [5,2] matmul V [2, 10] = [5, 10]</p>
    <p>- 4. After that calculate Attention,</p>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 400">
      <!-- Q matrix -->
      <rect x="50" y="50" width="100" height="50" fill="#FFB3BA" stroke="black" />
      <text x="100" y="80" text-anchor="middle" dominant-baseline="middle" font-size="10">Q [5x10]</text>

      <!-- K^T matrix -->
      <rect x="200" y="50" width="50" height="100" fill="#BAFFC9" stroke="black" />
      <text x="225" y="100" text-anchor="middle" dominant-baseline="middle" font-size="10">K^T [10x5]</text>

      <!-- QK^T result -->
      <rect x="300" y="50" width="50" height="50" fill="#BAE1FF" stroke="black" />
      <text x="325" y="80" text-anchor="middle" dominant-baseline="middle" font-size="10">QK^T [5x5]</text>

      <!-- Softmax -->
      <rect x="400" y="50" width="50" height="50" fill="#FFDFBA" stroke="black" />
      <text x="425" y="80" text-anchor="middle" dominant-baseline="middle" font-size="10">Softmax</text>

      <!-- V matrix -->
      <rect x="500" y="50" width="100" height="50" fill="#FFFFBA" stroke="black" />
      <text x="550" y="80" text-anchor="middle" dominant-baseline="middle" font-size="10">V [5x10]</text>

      <!-- Final result -->
      <rect x="650" y="50" width="100" height="50" fill="#E6BAFF" stroke="black" />
      <text x="700" y="80" text-anchor="middle" dominant-baseline="middle" font-size="10">Result [5x10]</text>

      <!-- Arrows -->
      <line x1="150" y1="75" x2="190" y2="75" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="250" y1="75" x2="290" y2="75" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="350" y1="75" x2="390" y2="75" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="450" y1="75" x2="490" y2="75" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
      <line x1="600" y1="75" x2="640" y2="75" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

      <!-- Arrowhead definition -->
      <defs>
        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
          <polygon points="0 0, 5 3.5, 0 7" />
        </marker>
      </defs>

      <!-- Labels -->
      <text x="175" y="60" text-anchor="middle" font-size="14">matmul</text>
      <text x="275" y="60" text-anchor="middle" font-size="14">matmul</text>
      <text x="375" y="60" text-anchor="middle" font-size="14">apply</text>
      <text x="475" y="60" text-anchor="middle" font-size="14">matmul</text>
      <text x="625" y="60" text-anchor="middle" font-size="14">matmul</text>

      <!-- Legend -->
      <text x="50" y="200" font-size="16" font-weight="bold">Legend:</text>
      <rect x="50" y="220" width="20" height="20" fill="#FFB3BA" stroke="black" />
      <text x="80" y="235" font-size="14">Q matrix</text>
      <rect x="50" y="250" width="20" height="20" fill="#BAFFC9" stroke="black" />
      <text x="80" y="265" font-size="14">K^T matrix</text>
      <rect x="50" y="280" width="20" height="20" fill="#BAE1FF" stroke="black" />
      <text x="80" y="295" font-size="14">QK^T result</text>
      <rect x="50" y="310" width="20" height="20" fill="#FFDFBA" stroke="black" />
      <text x="80" y="325" font-size="14">Softmax</text>
      <rect x="50" y="340" width="20" height="20" fill="#FFFFBA" stroke="black" />
      <text x="80" y="355" font-size="14">V matrix</text>
      <rect x="50" y="370" width="20" height="20" fill="#E6BAFF" stroke="black" />
      <text x="80" y="385" font-size="14">Final result</text>
    </svg>
    <p>The output shape should be [Q `L`, V `d_model`] = [5, 10]. To calculate the memory usage roughly <b>based on
        output
        shape</b>,</p>
    <p>- 1. Q, K and V linear weights, which each output is [in_d_model, d_model], <b>3 x in_d_model x d_model</b>.</p>
    <p>- 2. input matmul Q, K and V, which each output is [L, d_model], <b>3 x L x d_model</b>.</p>
    <p>- 3. softmax(QK^T)V, [L, d_model], <b>L x d_model</b>.</p>
    <p>- 4. Total, (3 x in_d_model x d_model) + (3 x L x d_model) + (L x d_model) = 260.</p>
    <p>- 7. Assumed we store in bfloat16, 260 x 2 = <b>520 bytes</b>.</p>

    <p>520 bytes is super small and yes that is for a simple example, but what if we use at least LLM 8B parameters such
      as Llama 3.1?</p>
    <h3>Use actual Llama 3.1 8B parameters</h3>
    <p>Based on the Llama 3.1 8B parameters settings from HuggingFace, <a
        href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/config.json">https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/config.json</a>,
      there are 3 settings important for attention size,
    <p>- 1. `hidden_size` = 4096.</p>
    <p>- 2. `num_attention_heads` = 32.</p>
    <p>Because Llama use multi-head attention and <b>to simplify the attention, assumed no group multi-head attention
        been used aka
        `num_key_value_heads`</b>, assume the input shape is [5, 4096], 5 sequence length with 4096 hidden size, so
      during calculating the attention,</p>
    <p>- 1. `head_dim` = hidden_size // num_attention_heads</p>
    <p>- 2. Q, K, V linear weights [hidden_size, num_attention_heads x head_dim], <b>3 x hidden_size x
        num_attention_heads x head_dim</b>.
    </p>
    <p>- 3. input matmul Q, K and V, which each output is [L, num_attention_heads x head_dim] and reshape become
      [num_attention_heads, L,
      head_dim], <b>3 x L x num_attention_heads x head_dim</b>.</p>
    <p>- 4. softmax(QK^T)V = [num_attention_heads, L, head_dim], <b>num_attention_heads x L x head_dim</b>.</p>
    <p>- 5. Total, (3 x hidden_size x num_attention_heads x head_dim) + (3 x L x num_attention_heads x head_dim) +
      (num_attention_heads x L x head_dim) = 50413568.
    </p>
    <p>- 6. Assumed we store in bfloat16, 50413568 x 2 = <b>100827136 bytes or 0.100827136 GB</b>, still small.</p>
    <p>Now what if you got 1M sequence length or 1M context length? replace the `L` with 1M, you got 16434331648
      bytes, saved as bfloat16, <b>16434331648 x 2 = 32868663296 bytes or 32.868663296 GB!</b></p>
    <p><b>32.868663296 GB</b> just for the attention, not included other linear layers and other matmul operations,
      insane. How about 13B or 70B parameters? kebabom!</p>
    <h3>Context Parallelism</h3>
    <p>When we talk about Parallelism in deep learning, it is about how to parallelize the data into multiple GPUs
      either to reduce computation burden and at the same reduce memory consumption or replicating the replica to
      increase the
      size of input to make learning process faster, and Context Parallelism is about
      how to parallelize the sequence length into multiple GPUs. Let say I have 2 GPUs, so the partition size is 2,</p>
    <div style="width: 60%">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 600 1000">
        <!-- Original Matrix -->
        <rect x="200" y="20" width="200" height="80" fill="#A0D8EF" stroke="black" />
        <text x="300" y="60" text-anchor="middle" font-size="14">Original Matrix</text>
        <text x="300" y="80" text-anchor="middle" font-size="12">[1, 1000000, 512]</text>

        <!-- Arrow -->
        <line x1="300" y1="100" x2="200" y2="150" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="300" y1="100" x2="400" y2="150" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Split Matrices -->
        <rect x="50" y="170" width="150" height="70" fill="#FFD700" stroke="black" />
        <text x="125" y="200" text-anchor="middle" font-size="14">GPU 1</text>
        <text x="125" y="220" text-anchor="middle" font-size="12">[1, 500000, 512]</text>

        <rect x="400" y="170" width="150" height="70" fill="#98FB98" stroke="black" />
        <text x="475" y="200" text-anchor="middle" font-size="14">GPU 2</text>
        <text x="475" y="220" text-anchor="middle" font-size="12">[1, 500000, 512]</text>

        <!-- Arrows to Local Attention -->
        <line x1="125" y1="240" x2="125" y2="290" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="475" y1="240" x2="475" y2="290" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Local Attention -->
        <rect x="50" y="310" width="150" height="70" fill="#FFD700" stroke="black" />
        <text x="125" y="340" text-anchor="middle" font-size="14">Local Attention</text>
        <text x="125" y="360" text-anchor="middle" font-size="12">Calculation</text>

        <rect x="400" y="310" width="150" height="70" fill="#98FB98" stroke="black" />
        <text x="475" y="340" text-anchor="middle" font-size="14">Local Attention</text>
        <text x="475" y="360" text-anchor="middle" font-size="12">Calculation</text>

        <!-- Arrows to Linear Layer -->
        <line x1="125" y1="380" x2="125" y2="430" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="475" y1="380" x2="475" y2="430" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Linear Layer -->
        <rect x="50" y="450" width="150" height="70" fill="#DDA0DD" stroke="black" />
        <text x="125" y="480" text-anchor="middle" font-size="14">Linear Layer</text>
        <text x="125" y="500" text-anchor="middle" font-size="12">Output Logits</text>

        <rect x="400" y="450" width="150" height="70" fill="#DDA0DD" stroke="black" />
        <text x="475" y="480" text-anchor="middle" font-size="14">Linear Layer</text>
        <text x="475" y="500" text-anchor="middle" font-size="12">Output Logits</text>

        <!-- Arrows to Loss Calculation -->
        <line x1="125" y1="520" x2="125" y2="570" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="475" y1="520" x2="475" y2="570" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Loss Calculation -->
        <rect x="50" y="590" width="150" height="70" fill="#F08080" stroke="black" />
        <text x="125" y="620" text-anchor="middle" font-size="14">Loss Calculation</text>
        <text x="125" y="640" text-anchor="middle" font-size="12">GPU 1 Loss</text>

        <rect x="400" y="590" width="150" height="70" fill="#F08080" stroke="black" />
        <text x="475" y="620" text-anchor="middle" font-size="14">Loss Calculation</text>
        <text x="475" y="640" text-anchor="middle" font-size="12">GPU 2 Loss</text>

        <!-- Arrows to Loss Gathering -->
        <line x1="125" y1="660" x2="125" y2="740" stroke="black" stroke-width="2" />
        <line x1="475" y1="660" x2="475" y2="740" stroke="black" stroke-width="2" />
        <line x1="125" y1="740" x2="300" y2="740" stroke="black" stroke-width="2" />
        <line x1="475" y1="740" x2="300" y2="740" stroke="black" stroke-width="2" />
        <line x1="300" y1="740" x2="300" y2="780" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Loss Gathering -->
        <rect x="200" y="800" width="200" height="80" fill="#90EE90" stroke="black" />
        <text x="300" y="830" text-anchor="middle" font-size="14">Gather Losses</text>
        <text x="300" y="850" text-anchor="middle" font-size="12">Average Loss</text>

        <!-- Labels -->
        <text x="300" y="920" text-anchor="middle" font-size="16" font-weight="bold">Context Parallelism</text>
        <text x="300" y="940" text-anchor="middle" font-size="14">Separate Logits and Loss</text>
        <text x="300" y="960" text-anchor="middle" font-size="14">Calculation per GPU</text>

        <!-- Arrowhead definition -->
        <defs>
          <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
            <polygon points="0 0, 10 3.5, 0 7" />
          </marker>
        </defs>
      </svg>
    </div>
    <p>So now each GPUs can calculate their own local attention but still coherent with the other local attentions and
      if you gather and combine the local attentions, the combined should be almost the same with the full attention
      with super super small different, and <b>you save GPU memory is saved by a factor of partition size!</b></p>

    <p>If we split the QKV into 2 GPUs, Q = [Q1, Q2], K = [K1, K2], V = [V1, V2], so local attentions,
      Attention1=softmax(Q1K1^T)V1
      and
      Attention2=softmax(Q2K2^T)V2.</p>
    <p><b>Now, how does softmax(Q1K1^T)V1 able to correlate with softmax(Q2K2^T)V2 ?</b> Especially on softmax, because
      softmax required sum of exponents on the hidden dimension.</p>

    <h3>Blockwise Parallel Transformer for Large Context Models</h3>
    <p>This paper <a href="https://arxiv.org/pdf/2305.19370">https://arxiv.org/pdf/2305.19370</a> shows that we can
      calculate Attention in blockwise manner on multiple devices.</p>
    <p>And this paper also mentioned `Self-attention can be computed in a blockwise manner without materializing the
      softmax
      attention
      matrix` which already done from <a href="https://arxiv.org/abs/2205.14135">Flash Attention: 2205.14135</a> and <a
        href="https://arxiv.org/abs/2112.05682">Self-attention does not need o(n2) memory: 2112.05682</a></p>
    <h4>Flash Attention</h4>
    <p>"Flash Attention" partitioned QKV into blocks inside the GPU and write in CUDA kernel and optimized the
      movement
      between GPU high bandwidth memory (HBM)
      and GPU on-chip SRAM, become more "io-awareness" by directly manipulating the memory hierarchy using CUDA
      interface. Flash Attention also
      calculate the attention using blockwise manner
      inside CUDA blocks.</p>
    <img src="/flash-attention.png" width="40%">
    <br>
    <img src="/flash-attention-1.png" width="40%">
    <p>- As you can see there are outer and inner loops, defined as, loop for each KV blocks, nested loop for each Q
      blocks, and calculate local max and local attention, gather local max to get global max and for each local
      attention
      minus with global max to get the global attention.</p>
    <h4>Self-attention does not need o(n2) memory</h4>
    <p>While "Self-attention does not need o(n2) memory" write using Jax to compute the blockwise, it is not as
      efficient as "Flash Attention" because Jax handled all the memories and there is no interface to make it
      "io-awareness" like "Flash Attention". The implementation,</p>
    <pre>
```python
import functools, jax, math
from jax import lax
from jax import numpy as jnp


def _query_chunk_attention(query,
                           key,
                           value,
                           key_chunk_size=4096,
                           precision=lax.Precision.HIGHEST,
                           dtype=jnp.float32):
  num_kv, num_heads, k_features = key.shape
  v_features = value.shape[-1]
  key_chunk_size = min(key_chunk_size, num_kv)
  query = query / jnp.sqrt(k_features).astype(dtype)

  @functools.partial(jax.checkpoint, prevent_cse=False)
  def summarize_chunk(query, key, value):
    attn_weights = jnp.einsum(
        'qhd,khd->qhk', query, key, precision=precision).astype(dtype)
    max_score = jnp.max(attn_weights, axis=-1, keepdims=True)
    max_score = jax.lax.stop_gradient(max_score)
    exp_weights = jnp.exp(attn_weights - max_score)
    exp_values = jnp.einsum(
        'vhf,qhv->qhf', value, exp_weights, precision=precision).astype(dtype)
    return (exp_values, exp_weights.sum(axis=-1),
            max_score.reshape((query.shape[0], num_heads)))

  def chunk_scanner(chunk_idx):
    key_chunk = lax.dynamic_slice(
        key, (chunk_idx, 0, 0),
        slice_sizes=(key_chunk_size, num_heads, k_features))
    value_chunk = lax.dynamic_slice(
        value, (chunk_idx, 0, 0),
        slice_sizes=(key_chunk_size, num_heads, v_features))
    return summarize_chunk(query, key_chunk, value_chunk)

  chunk_values, chunk_weights, chunk_max = lax.map(
      chunk_scanner, xs=jnp.arange(0, num_kv, key_chunk_size))

  global_max = jnp.max(chunk_max, axis=0, keepdims=True)
  max_diffs = jnp.exp(chunk_max - global_max)
  chunk_values *= jnp.expand_dims(max_diffs, axis=-1)
  chunk_weights *= max_diffs

  all_values = chunk_values.sum(axis=0)
  all_weights = jnp.expand_dims(chunk_weights, -1).sum(axis=0)
  return all_values / all_weights


def mefficient_attention(query,
                         key,
                         value,
                         query_chunk_size=1024,
                         precision=jax.lax.Precision.HIGHEST,
                         dtype=jnp.float32):
  num_q, num_heads, q_features = query.shape

  def chunk_scanner(chunk_idx, _):
    query_chunk = lax.dynamic_slice(
        query, (chunk_idx, 0, 0),
        slice_sizes=(min(query_chunk_size, num_q), num_heads, q_features))
    return (chunk_idx + query_chunk_size,
            _query_chunk_attention(
                query_chunk, key, value, precision=precision, dtype=dtype))

  _, res = lax.scan(
      chunk_scanner,
      init=0,
      xs=None,
      length=math.ceil(num_q / query_chunk_size))
  return res.reshape(num_q, num_heads, value.shape[-1])
```</pre>
    <p>- But basically is the same, loop Q blocks, loop nested KV blocks, and calculate local max and local attention,
      gather local max to get global max and for each local
      attention
      minus with global max to get the global attention.</p>
    <p>But "Flash Attention" and "Self-attention does not need o(n2) memory" partitioned the QKV into blocks happened
      inside
      a single GPU, not for multi-GPUs. And actually, <b>"Blockwise Parallel Transformer for Large Context
        Models" take inspiration directly from "Self-attention does not need o(n2) memory"</b>, but just do it on
      multi-GPUs level.</p>
    <p>In section 3, it stated Q can split into Bq blocks, and KV split into Bkv blocks, same as "Flash Attention" and
      "Self-attention does not need o(n2) memory".</p>
    <p>- 1. For each query block, the blockwise attention Attention(Qi, Kj, Vj) can be computed by iterating over all
      key-value
      blocks,</p>

    <div>
      $$
      \mathrm{Attention}(Q_i, K, V) = \mathrm{Scaling}(\{\exp(Q_i K_j^T)V_j\}_{j=1}^{B_{kv}})
      $$
    </div>
    <p>- 2. The scaling operation scales each blockwise attention based on the difference between the blockwise
      maximum and the global maximum.</p>
    <div>
      $$
      \mathrm{Attention}(Q_i, K_j, V_j) = \exp\bigl(Q_i K_j^T - \max(Q_i K_j^T)\bigr) / \sum \exp\bigl(Q_i K_j^T -
      \max(Q_i K_j^T)\bigr)
      $$
    </div>
    <div>
      $$
      \mathrm{max}_i = \max \bigl(\max(Q_i K_1^T), \ldots, \max(Q_i K_B^T)\bigr)
      $$
    </div>

    <p>- 3. Once the
      blockwise attention is computed, the global attention matrix can be obtained by scaling the blockwise
      attention using the difference between the blockwise and global softmax normalization constants.</p>
    <div>
      $$
      \mathrm{Attention}(Q_i, K, V) = \bigl[ \exp(Q_i K_j^T - \mathrm{max}_i)~\mathrm{Attention}(Q_i, K_j, V_j)
      \bigr]_{j=1}^{B_{kv}}
      $$
    </div>
    <p>- 4. But I believe there is a mistake to calculate <span>\(\mathrm{Attention}(Q_i, K, V)\),</span></p>
    <p>-- i. <span>\(Q_i K_j^T\)</span> shape is [L, L] while <span>\(\mathrm{Attention}(Q_i, K, V)\)</span> shape is
      [L, dim], so we cannot do hadamard product.</p>
    <p>-- ii. <span>It should be \(\exp(\max(Q_i K_j^T) - \mathrm{max}_i)\), so the shape will become [L]. When we do
        hadamard product, [L] o [L, dim], PyTorch will automatically repeat [L], [L, L, ...] become [L, dim] then we can
        do [L, dim] o [L, dim].</span></p>
    <p>--- iii. Actual equation should be, </p>
    <div>
      $$
      \mathrm{Attention}(Q_i, K, V) = \bigl[ \exp(\max(Q_i K_j^T) - \mathrm{max}_i)~\mathrm{Attention}(Q_i, K_j, V_j)
      \bigr]_{j=1}^{B_{kv}}
      $$
    </div>
    <p>Visualization to get for Attention(Qi, K, V),</p>
    <div style="width:60%">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 600 750">
        <!-- Background -->
        <rect width="600" height="800" fill="#fff" />

        <!-- Q block -->
        <rect x="250" y="50" width="100" height="60" fill="#ff9999" stroke="#000000" stroke-width="2" />
        <text x="300" y="85" text-anchor="middle" font-family="Arial, sans-serif" font-size="16">Qi</text>

        <!-- K-V blocks -->
        <rect x="100" y="150" width="180" height="60" fill="#99ff99" stroke="#000000" stroke-width="2" />
        <text x="190" y="185" text-anchor="middle" font-family="Arial, sans-serif" font-size="16">K0, V0</text>

        <rect x="320" y="150" width="180" height="60" fill="#99ff99" stroke="#000000" stroke-width="2" />
        <text x="410" y="185" text-anchor="middle" font-family="Arial, sans-serif" font-size="16">K1, V1</text>

        <!-- Attention computation -->
        <rect x="100" y="250" width="180" height="60" fill="#ffcc00" stroke="#000000" stroke-width="2" />
        <text x="190" y="285" text-anchor="middle" font-family="Arial, sans-serif"
          font-size="14">Attention(Qi,K0,V0)</text>

        <rect x="320" y="250" width="180" height="60" fill="#ffcc00" stroke="#000000" stroke-width="2" />
        <text x="410" y="285" text-anchor="middle" font-family="Arial, sans-serif"
          font-size="14">Attention(Qi,K1,V1)</text>

        <!-- Max operations -->
        <rect x="100" y="350" width="180" height="60" fill="#ff00ff" stroke="#000000" stroke-width="2" />
        <text x="190" y="385" text-anchor="middle" font-family="Arial, sans-serif" font-size="14">max(Qi,K0)</text>

        <rect x="320" y="350" width="180" height="60" fill="#ff00ff" stroke="#000000" stroke-width="2" />
        <text x="410" y="385" text-anchor="middle" font-family="Arial, sans-serif" font-size="14">max(Qi,K1)</text>

        <!-- Global max -->
        <rect x="210" y="450" width="180" height="60" fill="#ff8000" stroke="#000000" stroke-width="2" />
        <text x="300" y="485" text-anchor="middle" font-family="Arial, sans-serif" font-size="14">global_max</text>

        <!-- Scaled block attentions -->
        <rect x="100" y="550" width="180" height="60" fill="#00ffff" stroke="#000000" stroke-width="2" />
        <text x="190" y="575" text-anchor="middle" font-family="Arial, sans-serif" font-size="12">exp(max(Qi,K0) -
          global_max)</text>
        <text x="190" y="595" text-anchor="middle" font-family="Arial, sans-serif" font-size="12">*
          Attention(Qi,K0,V0)</text>

        <rect x="320" y="550" width="180" height="60" fill="#00ffff" stroke="#000000" stroke-width="2" />
        <text x="410" y="575" text-anchor="middle" font-family="Arial, sans-serif" font-size="12">exp(max(Qi,K1) -
          global_max)</text>
        <text x="410" y="595" text-anchor="middle" font-family="Arial, sans-serif" font-size="12">*
          Attention(Qi,K1,V1)</text>

        <!-- Final summed attention -->
        <rect x="210" y="650" width="180" height="60" fill="#ff9966" stroke="#000000" stroke-width="2" />
        <text x="300" y="685" text-anchor="middle" font-family="Arial, sans-serif" font-size="14">Sum of Scaled
          Attentions</text>

        <!-- Arrows -->
        <defs>
          <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
            <polygon points="0 0, 10 3.5, 0 7" />
          </marker>
        </defs>

        <!-- Qi to K1,V1 and K2,V2 -->
        <line x1="300" y1="110" x2="190" y2="140" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="300" y1="110" x2="410" y2="140" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- K1,V1 and K2,V2 to Attention computations -->
        <line x1="190" y1="210" x2="190" y2="240" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="410" y1="210" x2="410" y2="240" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Attention computations to max operations -->
        <line x1="190" y1="310" x2="190" y2="340" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="410" y1="310" x2="410" y2="340" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Max operations to global max -->
        <line x1="190" y1="410" x2="290" y2="440" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="410" y1="410" x2="310" y2="440" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Global max to scaled block attentions -->
        <line x1="300" y1="510" x2="210" y2="540" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="300" y1="510" x2="390" y2="540" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Attention computations to scaled block attentions -->
        <line x1="190" y1="310" x2="190" y2="540" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="410" y1="310" x2="410" y2="540" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Scaled block attentions to final summed attention -->
        <line x1="190" y1="610" x2="290" y2="640" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="410" y1="610" x2="310" y2="640" stroke="#000000" stroke-width="2" marker-end="url(#arrowhead)" />

        <!-- Final label -->
        <text x="300" y="740" text-anchor="middle" font-family="Arial, sans-serif" font-size="16">Final Output:
          Attention(Qi, K, V)</text>
      </svg>

    </div>
    <p>- 1. For Qi, loop for each KV blocks, j,</p>
    <p>-- i. Calculate maxj = max(QiKj^T) and Attentionj = softmax(QiKj^T - maxj)Vj</p>
    <p>-- ii. Store maxj in maxi, maxi = [maxj0, maxj1, ...] and Attentionj in Attentioni, Attentioni = [Attentionj0,
      Attentionj1, ...]</p>
    <p>- 2. Calculate max_maxi = max(maxi), loop again KV blocks, j,</p>
    <p>-- i. Calculate blockwisej = exp(maxj - max_maxi) x Attentionj, and store it in blockwisei, blockwisei =
      [blockwisej0, blockwisej1, ...]</p>
    <p>-- ii. Sum blockwisei, now you got Attention(Qi,K,V)!</p>
    <h3>PyTorch code using Loop</h3>
    <p>To test if it is working, we have to compare by doing full attention vs blockwise attention, after that we
      compare
      the full attention on the first partition size with the first blockwise attention,</p>
    <pre>
```python
import torch
import torch.nn.functional as F

Q = torch.randn(100, 128).cuda().to(torch.bfloat16)
K = torch.randn(100, 128).cuda().to(torch.bfloat16)
V = torch.randn(100, 128).cuda().to(torch.bfloat16)

full_attention = torch.matmul(F.softmax(torch.matmul(Q, K.T), dim = -1), V)

chunk_size = 2
Q_blocks = torch.chunk(Q, chunk_size)
K_blocks = torch.chunk(K, chunk_size)
V_blocks = torch.chunk(V, chunk_size)

Q_block = Q_blocks[0]
block_attentions = []
block_maxes = []

for K_block, V_block in zip(K_blocks, V_blocks):
    # Compute attention scores
    scores = torch.matmul(Q_block, K_block.T)

    # Compute block-wise max
    block_max = scores.max(dim=-1, keepdim=True)[0]
    block_maxes.append(block_max)

    # Compute block-wise attention
    block_attention = torch.matmul(F.softmax(scores - block_max, dim=-1), V_block)
    block_attentions.append(block_attention)

# Compute global max
global_max = torch.max(torch.cat(block_maxes, dim=-1), dim=-1, keepdim=True)[0]

# Scale and combine block attentions
scaled_attentions = [
    torch.exp(block_max - global_max) * block_attention
    for block_max, block_attention in zip(block_maxes, block_attentions)
]

output = sum(scaled_attentions)
```</pre>
    <p>Exact match signs,</p>
    <pre>
```python
(torch.sign(full_attention[:output.shape[0]]) == torch.sign(output)).float().mean()
```</pre>
    <p>The output,</p>
    <pre>
```text
tensor(0.9958, device='cuda:0')
```</pre>
    <p>Check different on `argmax(-1)`,</p>
    <pre>
```python
print(full_attention[:output.shape[0]].argmax(-1), output.argmax(-1))
```</pre>
    <p>Output,</p>
    <pre>
```text
tensor([122,  84,  27,  20,  98,  60,  36,  65,  39,  48,  31,  91,  48,  69,
         80,  98,  59, 121,   0,  24,  42,  67,  76,  58,  36,  34,  79,   1,
         57,  99,   9,  47,  77, 110,   9,   9, 119,   9,  34,  27,   6,  37,
        104, 121, 103, 123,   0,  56,  67, 104], device='cuda:0') 

tensor([122,  84,  27,  20,  98,  60,  36,  65,  39,  48,  31,  91,  48,  69,
         80,  98,  59, 121,   0,  24,  42,  39,  76,  58,  36,  34,  79,   1,
         57,  40,   9,  47,  77, 110,   9,   9, 119,   9,  34,  27,   6,  37,
        104, 121, 103, 123,   0,  56,  67, 104], device='cuda:0')
```</pre>
    <p>You can continue to run for Q blocks or Bq blocks. <b>As you can see, this blockwise is exactly as
        "Self-attention does not need o(n2) memory", just in PyTorch.</b></p>


    <h3>Use PyTorch distributed</h3>
    <p>Now we have to convert from loop execution to parallel execution using Torch Elastic Distributed, for me, if you
      want to do parallel execution, at first you must test it using loop execution, if it works, convert it to parallel
      execution.</p>
    <pre>
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import os

def main():
    world_size = torch.cuda.device_count()
    local_rank = int(os.environ["LOCAL_RANK"])
    device = f'cuda:{local_rank}'
    dist.init_process_group(backend='nccl')

    Q_block = torch.randn(50, 128).cuda(device=device).to(torch.bfloat16)
    K = torch.randn(50, 128).cuda(device=device).to(torch.bfloat16)
    V = torch.randn(50, 128).cuda(device=device).to(torch.bfloat16)

    block_attentions = []
    block_maxes = []

    for i in range(world_size):
        if i == local_rank:
            dist.broadcast(K, src=i)
            dist.broadcast(V, src=i)

            K_block = K
            V_block = V
        else:
            K_block = torch.empty_like(K)
            V_block = torch.empty_like(V)

            dist.broadcast(K_block, src=i)
            dist.broadcast(V_block, src=i)
        
        scores = torch.matmul(Q_block, K_block.T)
        block_max = scores.max(dim=-1, keepdim=True)[0]
        block_maxes.append(block_max)
        block_attention = torch.matmul(F.softmax(scores - block_max, dim=-1), V_block)
        block_attentions.append(block_attention)
    
    global_max = torch.max(torch.cat(block_maxes, dim=-1), dim=-1, keepdim=True)[0]

    scaled_attentions = [
        torch.exp(block_max - global_max) * block_attention
        for block_max, block_attention in zip(block_maxes, block_attentions)
    ]

    output = sum(scaled_attentions)
    print(local_rank, len(block_maxes), output.shape)

if __name__ == "__main__":
    main()
```</pre>
    <p>Save it as `context-parallelism.py`, and this example required minimum 2 GPUs, and to execute it using
      `torchrun`,</p>
    <pre>
```bash
torchrun \
--nproc-per-node=2 \
context-parallelism.py
```</pre>
    <p>The output,</p>
    <pre>
```
0 2 torch.Size([50, 128])
1 2 torch.Size([50, 128])
```</pre>
    <p>For each GPU able to get expected shape which is [50, 128], so the data flow is like,</p>
    <p>- 1. When we do context parallelism, each QKV blocks already initialized for each GPU, not during GPU 0 after
      that split to N GPUs, because GPU 0 it self not enough memory to chunks and scatter to N GPUs.</p>
    <p>- 2. We loop based on world size, if we got 2 GPUs, so the world size 2. If,</p>
    <p>-- i. If i equal to current device, `i == local_rank`, we have to broadcast KV blocks to other GPUs.</p>
    <p>-- ii. If i does not equal to current device, it means the local GPU must accept KV blocks from the other GPUs.
    </p>
    <p>-- iii. Calculate max(QiKj^T) and store it in block_maxes.</p>
    <p>-- iv. Calculate softmax(QiKj^T - max(QiKj^T))Vj and store it in block_attentions.</p>
    <p>- 3. Calculate the global_max from block_maxes.</p>
    <p>- 4. We iterate for each blocks from zip(block_maxes, block_attentions),</p>
    <p>-- i. Calculate exp(block_max - global_max) * block_attention and store in scaled_attentions.</p>
    <p>- 5. Sum scaled_attentions to get the blockwise attention at local GPU.</p>
    <p>- 6. The data movement is like below,</p>
    <div style="width: 70%">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 1100">
        <defs>
          <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
            <polygon points="0 0, 10 3.5, 0 7" fill="#000" />
          </marker>
        </defs>

        <!-- GPU 0 -->
        <rect x="50" y="50" width="300" height="1000" fill="#e6f3ff" stroke="#000" stroke-width="2" />
        <text x="200" y="40" text-anchor="middle" font-size="16" font-weight="bold">GPU 0</text>

        <!-- GPU 1 -->
        <rect x="450" y="50" width="300" height="1000" fill="#fff0e6" stroke="#000" stroke-width="2" />
        <text x="600" y="40" text-anchor="middle" font-size="16" font-weight="bold">GPU 1</text>

        <!-- Initial Data -->
        <text x="200" y="80" text-anchor="middle" font-size="14">Q0 K0 V0</text>
        <text x="600" y="80" text-anchor="middle" font-size="14">Q1 K1 V1</text>

        <!-- Data Broadcasting -->
        <line x1="200" y1="100" x2="600" y2="100" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <text x="400" y="95" text-anchor="middle" font-size="12">Broadcast K0, V0</text>

        <line x1="600" y1="120" x2="200" y2="120" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <text x="400" y="115" text-anchor="middle" font-size="12">Broadcast K1, V1</text>

        <!-- Block Operations for GPU 0 -->
        <text x="200" y="150" text-anchor="middle" font-size="14" font-weight="bold">Block Operations (GPU 0)</text>

        <rect x="70" y="170" width="260" height="80" fill="#d1e7ff" stroke="#000" stroke-width="1" />
        <text x="200" y="190" text-anchor="middle" font-size="12">Block 0 (i == local_rank)</text>
        <text x="200" y="210" text-anchor="middle" font-size="12">1. scores0 = Q0 * K0^T</text>
        <text x="200" y="230" text-anchor="middle" font-size="12">2. block_max0 = max(scores0)</text>

        <rect x="70" y="260" width="260" height="100" fill="#d1e7ff" stroke="#000" stroke-width="1" />
        <text x="200" y="280" text-anchor="middle" font-size="12">Block 1 (i != local_rank)</text>
        <text x="200" y="300" text-anchor="middle" font-size="12">1. scores1 = Q0 * K1^T</text>
        <text x="200" y="320" text-anchor="middle" font-size="12">2. block_max1 = max(scores1)</text>
        <text x="200" y="340" text-anchor="middle" font-size="12">3. Store block_max0, block_max1</text>

        <rect x="70" y="370" width="260" height="120" fill="#d1e7ff" stroke="#000" stroke-width="1" />
        <text x="200" y="390" text-anchor="middle" font-size="12">Attention Calculation</text>
        <text x="200" y="410" text-anchor="middle" font-size="12">1. softmax0 = softmax(scores0 - block_max0)</text>
        <text x="200" y="430" text-anchor="middle" font-size="12">2. attention0 = softmax0 * V0</text>
        <text x="200" y="450" text-anchor="middle" font-size="12">3. softmax1 = softmax(scores1 - block_max1)</text>
        <text x="200" y="470" text-anchor="middle" font-size="12">4. attention1 = softmax1 * V1</text>

        <!-- Block Operations for GPU 1 -->
        <text x="600" y="150" text-anchor="middle" font-size="14" font-weight="bold">Block Operations (GPU 1)</text>

        <rect x="470" y="170" width="260" height="80" fill="#ffd9b3" stroke="#000" stroke-width="1" />
        <text x="600" y="190" text-anchor="middle" font-size="12">Block 0 (i != local_rank)</text>
        <text x="600" y="210" text-anchor="middle" font-size="12">1. scores0 = Q1 * K0^T</text>
        <text x="600" y="230" text-anchor="middle" font-size="12">2. block_max0 = max(scores0)</text>

        <rect x="470" y="260" width="260" height="100" fill="#ffd9b3" stroke="#000" stroke-width="1" />
        <text x="600" y="280" text-anchor="middle" font-size="12">Block 1 (i == local_rank)</text>
        <text x="600" y="300" text-anchor="middle" font-size="12">1. scores1 = Q1 * K1^T</text>
        <text x="600" y="320" text-anchor="middle" font-size="12">2. block_max1 = max(scores1)</text>
        <text x="600" y="340" text-anchor="middle" font-size="12">3. Store block_max0, block_max1</text>

        <rect x="470" y="370" width="260" height="120" fill="#ffd9b3" stroke="#000" stroke-width="1" />
        <text x="600" y="390" text-anchor="middle" font-size="12">Attention Calculation</text>
        <text x="600" y="410" text-anchor="middle" font-size="12">1. softmax0 = softmax(scores0 - block_max0)</text>
        <text x="600" y="430" text-anchor="middle" font-size="12">2. attention0 = softmax0 * V0</text>
        <text x="600" y="450" text-anchor="middle" font-size="12">3. softmax1 = softmax(scores1 - block_max1)</text>
        <text x="600" y="470" text-anchor="middle" font-size="12">4. attention1 = softmax1 * V1</text>

        <!-- Global Max Calculation -->
        <rect x="70" y="510" width="260" height="60" fill="#d1e7ff" stroke="#000" stroke-width="1" />
        <text x="200" y="535" text-anchor="middle" font-size="12">Calculate global_max0 from</text>
        <text x="200" y="555" text-anchor="middle" font-size="12">local block_max0, block_max1</text>

        <rect x="470" y="510" width="260" height="60" fill="#ffd9b3" stroke="#000" stroke-width="1" />
        <text x="600" y="535" text-anchor="middle" font-size="12">Calculate global_max1 from</text>
        <text x="600" y="555" text-anchor="middle" font-size="12">local block_max0, block_max1</text>

        <!-- Scaled Attentions -->
        <rect x="70" y="590" width="260" height="140" fill="#d1e7ff" stroke="#000" stroke-width="1" />
        <text x="200" y="630" text-anchor="middle" font-size="12">1. scale0 = exp(block_max0 - global_max0)</text>
        <text x="200" y="650" text-anchor="middle" font-size="12">2. scale1 = exp(block_max1 - global_max0)</text>
        <text x="200" y="670" text-anchor="middle" font-size="12">3. scaled_attention0 = scale0 * attention0</text>
        <text x="200" y="690" text-anchor="middle" font-size="12">4. scaled_attention1 = scale1 * attention1</text>

        <rect x="470" y="590" width="260" height="140" fill="#ffd9b3" stroke="#000" stroke-width="1" />
        <text x="600" y="630" text-anchor="middle" font-size="12">1. scale0 = exp(block_max0 - global_max1)</text>
        <text x="600" y="650" text-anchor="middle" font-size="12">2. scale1 = exp(block_max1 - global_max1)</text>
        <text x="600" y="670" text-anchor="middle" font-size="12">3. scaled_attention0 = scale0 * attention0</text>
        <text x="600" y="690" text-anchor="middle" font-size="12">4. scaled_attention1 = scale1 * attention1</text>

        <!-- Final Sum -->
        <rect x="70" y="750" width="260" height="60" fill="#d1e7ff" stroke="#000" stroke-width="1" />

        <text x="200" y="790" text-anchor="middle" font-size="12">output0 = scaled_attention0 + scaled_attention1</text>

        <rect x="470" y="750" width="260" height="60" fill="#ffd9b3" stroke="#000" stroke-width="1" />

        <text x="600" y="790" text-anchor="middle" font-size="12">output1 = scaled_attention0 + scaled_attention1</text>

        <!-- Continue Processing -->
        <text x="400" y="840" text-anchor="middle" font-size="14" font-weight="bold">Continue downstream
          processing</text>
        <line x1="200" y1="860" x2="200" y2="1020" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
        <line x1="600" y1="860" x2="600" y2="1020" stroke="#000" stroke-width="2" marker-end="url(#arrowhead)" />
      </svg>
    </div>

    <h3>Improvement</h3>
    <p><a href="https://arxiv.org/abs/2310.01889">Ring Attention: 2310.01889</a> from the same authors to improve this
      Blockwise Attention by simply reduce the communication between nodes by using ring communication.</p>
    <p>And recently there is <a href="https://arxiv.org/abs/2408.04093">Tree Attention: 2408.04093</a> to improve Ring
      Attention by aggregating the max(KV.T) on tree hierarchy, which is only make sense for multi-nodes.</p>

    <p>---</p>
    <p>thats all, give some love to <a href="https://x.com/aisyahhhrzk" target="_blank">Aisyah Razak.</a></p>
  </div>
</body>

</html>